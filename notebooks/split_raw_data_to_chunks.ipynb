{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "from src.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    data = []\n",
    "\n",
    "    for i in itertools.count(start=1):\n",
    "        try:\n",
    "            text = open(os.path.join(path, 'problem-' + str(i) + '.txt'), 'r').read()\n",
    "            changes = json.load(open(os.path.join(path, 'problem-' + str(i) + '.truth')))\n",
    "            data.append(Document(text, **changes))\n",
    "        except FileNotFoundError:\n",
    "            break\n",
    "    \n",
    "    return data\n",
    "\n",
    "train = get_data('../data/train_raw')\n",
    "validation = get_data('../data/validation_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split each text accordingly:\n",
    "    - split at given split positions if text has multiple authors\n",
    "    - split to 3 chunks with equal length otherwise\n",
    "    \n",
    "Each row is a tuple of (first_text_chunk, second_text_chunk, different_author(T/F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_chunks_csv(documents, csv_out):\n",
    "    new_train = []\n",
    "\n",
    "    for d in documents:\n",
    "        indices = []\n",
    "        if not d.has_changes:\n",
    "            indices = [None, len(d.sentences)//3, (len(d.sentences)//3)*2, None]\n",
    "        else:\n",
    "            indices = [None] + d.sent_positions + [None]\n",
    "\n",
    "        cache = {}\n",
    "        for i, j in zip(indices[:-1], indices[1:]):\n",
    "            cache['{} {}'.format(i,j)] = ' '.join(d.sentences[i:j])\n",
    "\n",
    "        rows = [\n",
    "            (\n",
    "                cache['{} {}'.format(i,j)],\n",
    "                cache['{} {}'.format(j,k)],\n",
    "                d.has_changes\n",
    "            ) for i, j, k in zip(indices[:-2], indices[1:-1], indices[2:])\n",
    "        ]\n",
    "\n",
    "        new_train.extend(rows)\n",
    "\n",
    "    with open(csv_out, 'w') as out:\n",
    "        csv_out = csv.writer(out)\n",
    "        for row in new_train:\n",
    "            csv_out.writerow(row)\n",
    "            \n",
    "            \n",
    "train_path = '../data/train_chunks.csv'\n",
    "if not os.path.exists(train_path):\n",
    "    gen_chunks_csv(train, train_path)\n",
    "    \n",
    "val_path = '../data/validation_chunks.csv'\n",
    "if not os.path.exists(val_path):\n",
    "    gen_chunks_csv(validation, val_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
