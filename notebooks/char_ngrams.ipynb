{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "from src.text_chunk import TextChunk\n",
    "from src.stylometry_extractor import StylometryExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "data = []\n",
    "path = '../data/train_raw'\n",
    "\n",
    "for i in itertools.count(start=1):\n",
    "    try:\n",
    "        text = open(os.path.join(path, 'problem-' + str(i) + '.txt'), 'r').read()\n",
    "        data.append(TextChunk(text))\n",
    "    except FileNotFoundError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function <lambda> at 0x122fdd950>, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "split_by = StylometryExtractor.SPECIAL_CHAR\n",
    "\n",
    "# NOTE: passing split_by because otherwise when \"unpickled\" it *must* exist in python's symbol table\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x, s=split_by: x.split(s),\n",
    "                             max_features=100,\n",
    "                             stop_words=stopwords.words('english'))\n",
    "\n",
    "vectorizer.fit(map(lambda text: text.ngram_string, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'king': 44,\n",
       " 'esti': 26,\n",
       " 'able': 0,\n",
       " 'rati': 73,\n",
       " 'atio': 9,\n",
       " 'tion': 93,\n",
       " 'rent': 78,\n",
       " 'ject': 43,\n",
       " 'ence': 20,\n",
       " 'ting': 92,\n",
       " 'ions': 40,\n",
       " 'pres': 70,\n",
       " 'enti': 21,\n",
       " 'make': 51,\n",
       " 'nder': 55,\n",
       " 'stan': 84,\n",
       " 'ding': 17,\n",
       " 'ange': 5,\n",
       " 'athe': 8,\n",
       " 'ther': 88,\n",
       " 'spec': 83,\n",
       " 'lati': 46,\n",
       " 'like': 47,\n",
       " 'ance': 4,\n",
       " 'comm': 11,\n",
       " 'ment': 53,\n",
       " 'ents': 22,\n",
       " 'ight': 36,\n",
       " 'need': 56,\n",
       " 'comp': 12,\n",
       " 'mple': 54,\n",
       " 'ning': 57,\n",
       " 'stor': 87,\n",
       " 'cont': 14,\n",
       " 'ling': 48,\n",
       " 'even': 27,\n",
       " 'ctio': 15,\n",
       " 'iona': 39,\n",
       " 'onal': 59,\n",
       " 'work': 98,\n",
       " 'part': 66,\n",
       " 'itio': 42,\n",
       " 'ible': 33,\n",
       " 'sion': 82,\n",
       " 'ated': 7,\n",
       " 'real': 75,\n",
       " 'ally': 2,\n",
       " 'ring': 79,\n",
       " 'time': 91,\n",
       " 'ecti': 18,\n",
       " 'tive': 94,\n",
       " 'ture': 96,\n",
       " 'ical': 34,\n",
       " 'also': 3,\n",
       " 'woul': 99,\n",
       " 'ould': 63,\n",
       " 'sing': 81,\n",
       " 'ever': 28,\n",
       " 'thin': 89,\n",
       " 'hing': 32,\n",
       " 'prob': 71,\n",
       " 'ound': 64,\n",
       " 'ings': 37,\n",
       " 'read': 74,\n",
       " 'emen': 19,\n",
       " 'side': 80,\n",
       " 'port': 69,\n",
       " 'reat': 77,\n",
       " 'call': 10,\n",
       " 'cons': 13,\n",
       " 'tter': 95,\n",
       " 'know': 45,\n",
       " 'form': 31,\n",
       " 'reas': 76,\n",
       " 'inte': 38,\n",
       " 'nter': 58,\n",
       " 'thou': 90,\n",
       " 'ough': 62,\n",
       " 'llow': 50,\n",
       " 'acti': 1,\n",
       " 'peop': 67,\n",
       " 'eopl': 23,\n",
       " 'ople': 60,\n",
       " 'lity': 49,\n",
       " 'ount': 65,\n",
       " 'fere': 29,\n",
       " 'ques': 72,\n",
       " 'uest': 97,\n",
       " 'stio': 86,\n",
       " 'arti': 6,\n",
       " 'eren': 25,\n",
       " 'isti': 41,\n",
       " 'eral': 24,\n",
       " 'othe': 61,\n",
       " 'mean': 52,\n",
       " 'diff': 16,\n",
       " 'iffe': 35,\n",
       " 'ffer': 30,\n",
       " 'stat': 85,\n",
       " 'play': 68}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open('../data/vectorizer.pk', 'wb') as f:\n",
    "    dill.dump(vectorizer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
