{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "from src.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    data = []\n",
    "    for i in itertools.count(start=1):\n",
    "        try:\n",
    "            text = open(os.path.join(path, 'problem-' + str(i) + '.txt'), 'r').read()\n",
    "            changes = json.load(open(os.path.join(path, 'problem-' + str(i) + '.truth')))\n",
    "            data.append(Document(text, **changes))\n",
    "        except FileNotFoundError:\n",
    "            break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data('../data/train_raw')\n",
    "validation = get_data('../data/validation_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 300\n",
    "embeddings_index = {}\n",
    "with open(os.path.expanduser('~/Downloads/glove.6B/glove.6B.{}d.txt'.format(embedding_size))) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = lambda w: embeddings_index[w] if w in embeddings_index else np.zeros(embedding_size, dtype='float32')\n",
    "def word2vec(w):\n",
    "#     if w[0] in set('ABCDEFGHIJKLMNOPQRSTUVWXYZ'):\n",
    "#         print(w)\n",
    "    w = w.lower()\n",
    "    if w in embeddings_index:\n",
    "        return embeddings_index[w]\n",
    "    return np.zeros(embedding_size, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2X_y(documents):\n",
    "    def minmax(a, b):\n",
    "        return sum(np.minimum(a, b)) / sum(np.maximum(a, b))\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for d in documents:\n",
    "        indices = []\n",
    "        if not d.has_changes:\n",
    "#             indices = [None, len(d.sentences)//3, (len(d.sentences)//3)*2, None]\n",
    "            indices = [None, len(d.words)//3, (len(d.words)//3)*2, None]\n",
    "        else:\n",
    "#             indices = [None] + d.sent_positions + [None]\n",
    "            indices = [None] + d.word_positions + [None]\n",
    "\n",
    "        cache = {}\n",
    "        for i, j in zip(indices[:-1], indices[1:]):\n",
    "#             cache['{} {}'.format(i,j)] = ' '.join(d.sentences[i:j])\n",
    "#             cache['{} {}'.format(i,j)] = np.mean(list(\n",
    "#                 map(lambda s: np.mean(list(map(word2vec, word_tokenize(s))), axis=0),\n",
    "#                     d.sentences[i:j])),axis=0)\n",
    "            v = np.sum(list(map(word2vec, d.words[i:j])), axis=0)\n",
    "#             v = v / np.linalg.norm(v)\n",
    "            cache['{} {}'.format(i,j)] = v\n",
    "\n",
    "        for i, j, k in zip(indices[:-2], indices[1:-1], indices[2:]):\n",
    "            a = cache['{} {}'.format(i,j)]\n",
    "            b = cache['{} {}'.format(j,k)]\n",
    "            X.append(np.array([\n",
    "                minmax(a, b),\n",
    "                distance.cosine(a, b),\n",
    "                distance.braycurtis(a, b),\n",
    "                distance.canberra(a, b),\n",
    "                distance.cityblock(a, b)\n",
    "            ]))\n",
    "            y.append(d.has_changes)\n",
    "    return np.asarray(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = docs2X_y(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = docs2X_y(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', SVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9029800203183204"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8939965694682676"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
