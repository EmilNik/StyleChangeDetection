{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine, braycurtis, canberra, cityblock, chebyshev, minkowski\n",
    "\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NUMBER_OF_FEATURES = 83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/500_ngrams/train_features_scaled_minmax.csv', index_col=0)\n",
    "val = pd.read_csv('../data/500_ngrams/validation_features_scaled_minmax.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_0</th>\n",
       "      <th>A_1</th>\n",
       "      <th>A_2</th>\n",
       "      <th>A_3</th>\n",
       "      <th>A_4</th>\n",
       "      <th>A_5</th>\n",
       "      <th>A_6</th>\n",
       "      <th>A_7</th>\n",
       "      <th>A_8</th>\n",
       "      <th>A_9</th>\n",
       "      <th>...</th>\n",
       "      <th>B_926</th>\n",
       "      <th>B_927</th>\n",
       "      <th>B_928</th>\n",
       "      <th>B_929</th>\n",
       "      <th>B_930</th>\n",
       "      <th>B_931</th>\n",
       "      <th>B_932</th>\n",
       "      <th>B_933</th>\n",
       "      <th>B_934</th>\n",
       "      <th>different_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.591167</td>\n",
       "      <td>0.238369</td>\n",
       "      <td>0.393659</td>\n",
       "      <td>0.654621</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.544193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184063</td>\n",
       "      <td>0.470222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072784</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500083</td>\n",
       "      <td>0.186608</td>\n",
       "      <td>0.325804</td>\n",
       "      <td>0.428397</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.448047</td>\n",
       "      <td>0.121928</td>\n",
       "      <td>0.203754</td>\n",
       "      <td>0.409647</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.589960</td>\n",
       "      <td>0.255750</td>\n",
       "      <td>0.393005</td>\n",
       "      <td>0.653535</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.552540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184063</td>\n",
       "      <td>0.469346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.519795</td>\n",
       "      <td>0.224352</td>\n",
       "      <td>0.295311</td>\n",
       "      <td>0.506321</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.553020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114050</td>\n",
       "      <td>0.482341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.616025</td>\n",
       "      <td>0.304250</td>\n",
       "      <td>0.248293</td>\n",
       "      <td>0.639090</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.629050</td>\n",
       "      <td>0.048339</td>\n",
       "      <td>0.173098</td>\n",
       "      <td>0.417508</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1871 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        A_0       A_1       A_2       A_3   A_4       A_5       A_6       A_7  \\\n",
       "0  0.591167  0.238369  0.393659  0.654621  0.23  0.544193  0.000000  0.184063   \n",
       "1  0.500083  0.186608  0.325804  0.428397  0.00  0.448047  0.121928  0.203754   \n",
       "2  0.589960  0.255750  0.393005  0.653535  0.23  0.552540  0.000000  0.184063   \n",
       "3  0.519795  0.224352  0.295311  0.506321  0.00  0.553020  0.000000  0.114050   \n",
       "4  0.616025  0.304250  0.248293  0.639090  0.00  0.629050  0.048339  0.173098   \n",
       "\n",
       "        A_8       A_9        ...         B_926  B_927  B_928  B_929     B_930  \\\n",
       "0  0.470222  0.000000        ...           0.0    0.0    0.0    0.0  0.095192   \n",
       "1  0.409647  0.069231        ...           0.0    0.0    0.0    0.0  0.742500   \n",
       "2  0.469346  0.000000        ...           0.0    0.0    0.0    0.0  0.000000   \n",
       "3  0.482341  0.000000        ...           0.0    0.0    0.0    0.0  0.247191   \n",
       "4  0.417508  0.134831        ...           0.0    0.0    0.0    0.0  0.081481   \n",
       "\n",
       "   B_931  B_932  B_933     B_934  different_author  \n",
       "0    0.0    0.0    0.0  0.072784              True  \n",
       "1    0.0    0.0    0.0  0.000000              True  \n",
       "2    0.0    0.0    0.0  0.000000              True  \n",
       "3    0.0    0.0    0.0  0.000000             False  \n",
       "4    0.0    0.0    0.0  0.000000             False  \n",
       "\n",
       "[5 rows x 1871 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_max_similarity(vectors):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    \n",
    "    for i in range(NUMBER_OF_FEATURES):\n",
    "        a_feature = vectors['A_{}'.format(i)]\n",
    "        b_feature = vectors['B_{}'.format(i)]\n",
    "        numerator += min(a_feature, b_feature)\n",
    "        denominator += max(a_feature, b_feature)\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def cosine_similarity(vectors):\n",
    "    a = [vectors['A_{}'.format(i)] for i in range(NUMBER_OF_FEATURES)]\n",
    "    b = [vectors['B_{}'.format(i)] for i in range(NUMBER_OF_FEATURES)]\n",
    "\n",
    "    return 1 - cosine(a, b)\n",
    "    \n",
    "train['min_max_similarity'] = train.apply(lambda vectors: min_max_similarity(vectors[:-1]), axis=1)\n",
    "val['min_max_similarity'] = val.apply(lambda vectors: min_max_similarity(vectors[:-1]), axis=1)\n",
    "\n",
    "train['cos_similarity'] = train.apply(lambda vectors: cosine_similarity(vectors[:-1]), axis=1)\n",
    "val['cos_similarity'] = val.apply(lambda vectors: cosine_similarity(vectors[:-1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_0</th>\n",
       "      <th>A_1</th>\n",
       "      <th>A_2</th>\n",
       "      <th>A_3</th>\n",
       "      <th>A_4</th>\n",
       "      <th>A_5</th>\n",
       "      <th>A_6</th>\n",
       "      <th>A_7</th>\n",
       "      <th>A_8</th>\n",
       "      <th>A_9</th>\n",
       "      <th>...</th>\n",
       "      <th>B_826</th>\n",
       "      <th>B_827</th>\n",
       "      <th>B_828</th>\n",
       "      <th>B_829</th>\n",
       "      <th>B_830</th>\n",
       "      <th>B_831</th>\n",
       "      <th>B_832</th>\n",
       "      <th>different_author</th>\n",
       "      <th>min_max_similarity</th>\n",
       "      <th>cos_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.591167</td>\n",
       "      <td>0.238369</td>\n",
       "      <td>0.393659</td>\n",
       "      <td>0.654621</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.544193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.470222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072784</td>\n",
       "      <td>True</td>\n",
       "      <td>0.341655</td>\n",
       "      <td>0.670623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500083</td>\n",
       "      <td>0.186608</td>\n",
       "      <td>0.325804</td>\n",
       "      <td>0.428397</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.448047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409647</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.339214</td>\n",
       "      <td>0.668914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.589960</td>\n",
       "      <td>0.255750</td>\n",
       "      <td>0.393005</td>\n",
       "      <td>0.653535</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.552540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.319101</td>\n",
       "      <td>0.626929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.519795</td>\n",
       "      <td>0.224352</td>\n",
       "      <td>0.295311</td>\n",
       "      <td>0.506321</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.553020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.366793</td>\n",
       "      <td>0.708512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.616025</td>\n",
       "      <td>0.304250</td>\n",
       "      <td>0.248293</td>\n",
       "      <td>0.639090</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.629050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.417508</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.416457</td>\n",
       "      <td>0.773555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1669 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        A_0       A_1       A_2       A_3   A_4       A_5  A_6  A_7       A_8  \\\n",
       "0  0.591167  0.238369  0.393659  0.654621  0.23  0.544193  0.0  0.0  0.470222   \n",
       "1  0.500083  0.186608  0.325804  0.428397  0.00  0.448047  0.0  0.0  0.409647   \n",
       "2  0.589960  0.255750  0.393005  0.653535  0.23  0.552540  0.0  0.0  0.469346   \n",
       "3  0.519795  0.224352  0.295311  0.506321  0.00  0.553020  0.0  0.0  0.482341   \n",
       "4  0.616025  0.304250  0.248293  0.639090  0.00  0.629050  0.0  0.0  0.417508   \n",
       "\n",
       "        A_9       ...        B_826  B_827     B_828  B_829  B_830  B_831  \\\n",
       "0  0.000000       ...          0.0    0.0  0.095192    0.0    0.0    0.0   \n",
       "1  0.069231       ...          0.0    0.0  0.742500    0.0    0.0    0.0   \n",
       "2  0.000000       ...          0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "3  0.000000       ...          0.0    0.0  0.247191    0.0    0.0    0.0   \n",
       "4  0.134831       ...          0.0    0.0  0.081481    0.0    0.0    0.0   \n",
       "\n",
       "      B_832  different_author  min_max_similarity  cos_similarity  \n",
       "0  0.072784              True            0.341655        0.670623  \n",
       "1  0.000000              True            0.339214        0.668914  \n",
       "2  0.000000              True            0.319101        0.626929  \n",
       "3  0.000000             False            0.366793        0.708512  \n",
       "4  0.000000             False            0.416457        0.773555  \n",
       "\n",
       "[5 rows x 1669 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.338874\n",
       "True     0.319502\n",
       "Name: min_max_similarity, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('different_author').mean()['min_max_similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.337823\n",
       "True     0.321173\n",
       "Name: min_max_similarity, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.groupby('different_author').mean()['min_max_similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.28000000000000003: 0.50846596681341005,\n",
       " 0.28050000000000003: 0.50711141212326449,\n",
       " 0.28100000000000003: 0.50677277345072802,\n",
       " 0.28150000000000003: 0.50812732814087369,\n",
       " 0.28200000000000003: 0.50880460548594653,\n",
       " 0.28250000000000003: 0.5099898408398239,\n",
       " 0.28300000000000003: 0.50982052150355572,\n",
       " 0.28350000000000003: 0.51083643752116492,\n",
       " 0.28400000000000003: 0.51236031154757877,\n",
       " 0.28450000000000003: 0.5130375888926515,\n",
       " 0.28500000000000003: 0.51286826955638332,\n",
       " 0.28550000000000003: 0.51523874026413818,\n",
       " 0.28600000000000003: 0.51557737893667455,\n",
       " 0.28650000000000003: 0.51642397561801556,\n",
       " 0.28700000000000003: 0.51693193362682022,\n",
       " 0.28750000000000003: 0.5171012529630884,\n",
       " 0.28800000000000003: 0.51862512698950225,\n",
       " 0.28850000000000003: 0.51964104300711145,\n",
       " 0.28900000000000003: 0.52065695902472064,\n",
       " 0.28950000000000004: 0.52099559769725701,\n",
       " 0.29000000000000004: 0.52218083305113439,\n",
       " 0.29050000000000004: 0.52218083305113439,\n",
       " 0.29100000000000004: 0.52336606840501187,\n",
       " 0.29150000000000004: 0.52353538774128006,\n",
       " 0.29200000000000004: 0.52455130375888925,\n",
       " 0.29250000000000004: 0.52607517778530311,\n",
       " 0.29300000000000004: 0.52776837114798514,\n",
       " 0.29350000000000004: 0.52929224517439888,\n",
       " 0.29400000000000004: 0.52980020318320353,\n",
       " 0.29450000000000004: 0.53081611920081273,\n",
       " 0.29500000000000004: 0.53166271588215375,\n",
       " 0.29550000000000004: 0.5321706738909584,\n",
       " 0.29600000000000004: 0.53454114459871316,\n",
       " 0.29650000000000004: 0.53657297663393155,\n",
       " 0.29700000000000004: 0.53758889265154075,\n",
       " 0.29750000000000004: 0.53877412800541824,\n",
       " 0.29800000000000004: 0.54063664070436845,\n",
       " 0.29850000000000004: 0.54249915340331867,\n",
       " 0.29900000000000004: 0.5441923467660007,\n",
       " 0.29950000000000004: 0.54486962411107354,\n",
       " 0.30000000000000004: 0.54825601083643749,\n",
       " 0.30050000000000004: 0.54944124619031498,\n",
       " 0.30100000000000005: 0.550287842871656,\n",
       " 0.30150000000000005: 0.55130375888926519,\n",
       " 0.30200000000000005: 0.55401286826955642,\n",
       " 0.30250000000000005: 0.55485946495089744,\n",
       " 0.30300000000000005: 0.55672197764984765,\n",
       " 0.30350000000000005: 0.5582458516762614,\n",
       " 0.30400000000000005: 0.56112428039282081,\n",
       " 0.30450000000000005: 0.56332543176430749,\n",
       " 0.30500000000000005: 0.56400270910938033,\n",
       " 0.30550000000000005: 0.56654249915340327,\n",
       " 0.30600000000000005: 0.56840501185235359,\n",
       " 0.30650000000000005: 0.56874365052488995,\n",
       " 0.30700000000000005: 0.56891296986115814,\n",
       " 0.30750000000000005: 0.5692516085336945,\n",
       " 0.30800000000000005: 0.5722993565865222,\n",
       " 0.30850000000000005: 0.57416186928547241,\n",
       " 0.30900000000000005: 0.57466982729427696,\n",
       " 0.30950000000000005: 0.57619370132069081,\n",
       " 0.31000000000000005: 0.57788689468337284,\n",
       " 0.31050000000000005: 0.57805621401964102,\n",
       " 0.31100000000000005: 0.57907213003725022,\n",
       " 0.31150000000000005: 0.58025736539112771,\n",
       " 0.31200000000000006: 0.58076532339993225,\n",
       " 0.31250000000000006: 0.58127328140873691,\n",
       " 0.31300000000000006: 0.58127328140873691,\n",
       " 0.31350000000000006: 0.58313579410768712,\n",
       " 0.31400000000000006: 0.58398239078902814,\n",
       " 0.31450000000000006: 0.5843210294615645,\n",
       " 0.31500000000000006: 0.58499830680663734,\n",
       " 0.31550000000000006: 0.58618354216051471,\n",
       " 0.31600000000000006: 0.58669150016931937,\n",
       " 0.31650000000000006: 0.5853369454791737,\n",
       " 0.31700000000000006: 0.58669150016931937,\n",
       " 0.31750000000000006: 0.58669150016931937,\n",
       " 0.31800000000000006: 0.58770741618692857,\n",
       " 0.31850000000000006: 0.58889265154080594,\n",
       " 0.31900000000000006: 0.58770741618692857,\n",
       " 0.31950000000000006: 0.58872333220453776,\n",
       " 0.32000000000000006: 0.59058584490348798,\n",
       " 0.32050000000000006: 0.59210971892990183,\n",
       " 0.32100000000000006: 0.59312563494751103,\n",
       " 0.32150000000000006: 0.59261767693870637,\n",
       " 0.32200000000000006: 0.59261767693870637,\n",
       " 0.32250000000000006: 0.59312563494751103,\n",
       " 0.32300000000000006: 0.59278699627497455,\n",
       " 0.32350000000000007: 0.59380291229258386,\n",
       " 0.32400000000000007: 0.59515746698272942,\n",
       " 0.32450000000000007: 0.59549610565526578,\n",
       " 0.32500000000000007: 0.59668134100914327,\n",
       " 0.32550000000000007: 0.5993904503894345,\n",
       " 0.32600000000000007: 0.60108364375211654,\n",
       " 0.32650000000000007: 0.60345411445987129,\n",
       " 0.32700000000000007: 0.60463934981374878,\n",
       " 0.32750000000000007: 0.6063325431764307,\n",
       " 0.32800000000000007: 0.60548594649508969,\n",
       " 0.32850000000000007: 0.60667118184896718,\n",
       " 0.32900000000000007: 0.60667118184896718,\n",
       " 0.32950000000000007: 0.60700982052150354,\n",
       " 0.33000000000000007: 0.60717913985777172,\n",
       " 0.33050000000000007: 0.60904165255672194,\n",
       " 0.33100000000000007: 0.60971892990179477,\n",
       " 0.33150000000000007: 0.61073484591940397,\n",
       " 0.33200000000000007: 0.61073484591940397,\n",
       " 0.33250000000000007: 0.61073484591940397,\n",
       " 0.33300000000000007: 0.61039620724686761,\n",
       " 0.33350000000000007: 0.61022688791059942,\n",
       " 0.33400000000000007: 0.61090416525567215,\n",
       " 0.33450000000000008: 0.6114121232644768,\n",
       " 0.33500000000000008: 0.61225871994581782,\n",
       " 0.33550000000000008: 0.61158144260074498,\n",
       " 0.33600000000000008: 0.61192008127328146,\n",
       " 0.33650000000000008: 0.61225871994581782,\n",
       " 0.33700000000000008: 0.61361327463596338,\n",
       " 0.33750000000000008: 0.61361327463596338,\n",
       " 0.33800000000000008: 0.61192008127328146,\n",
       " 0.33850000000000008: 0.61259735861835418,\n",
       " 0.33900000000000008: 0.61327463596342702,\n",
       " 0.33950000000000008: 0.61361327463596338,\n",
       " 0.34000000000000008: 0.61327463596342702,\n",
       " 0.34050000000000008: 0.61327463596342702,\n",
       " 0.34100000000000008: 0.61327463596342702,\n",
       " 0.34150000000000008: 0.61479850998984087,\n",
       " 0.34200000000000008: 0.61462919065357269,\n",
       " 0.34250000000000008: 0.61615306467998643,\n",
       " 0.34300000000000008: 0.61733830003386392,\n",
       " 0.34350000000000008: 0.61564510667118189,\n",
       " 0.34400000000000008: 0.61479850998984087,\n",
       " 0.34450000000000008: 0.61293599729089066,\n",
       " 0.34500000000000008: 0.61429055198103621,\n",
       " 0.34550000000000008: 0.61496782932610905,\n",
       " 0.34600000000000009: 0.61581442600745007,\n",
       " 0.34650000000000009: 0.61666102268879108,\n",
       " 0.34700000000000009: 0.61666102268879108,\n",
       " 0.34750000000000009: 0.61801557737893664,\n",
       " 0.34800000000000009: 0.61683034202505926,\n",
       " 0.34850000000000009: 0.61666102268879108,\n",
       " 0.34900000000000009: 0.61666102268879108,\n",
       " 0.34950000000000009: 0.61767693870640028,\n",
       " 0.35000000000000009: 0.61632238401625461,\n",
       " 0.35050000000000009: 0.61615306467998643,\n",
       " 0.35100000000000009: 0.61632238401625461,\n",
       " 0.35150000000000009: 0.61683034202505926,\n",
       " 0.35200000000000009: 0.61530646799864541,\n",
       " 0.35250000000000009: 0.61462919065357269,\n",
       " 0.35300000000000009: 0.61395191330849985,\n",
       " 0.35350000000000009: 0.61259735861835418,\n",
       " 0.35400000000000009: 0.60971892990179477,\n",
       " 0.35450000000000009: 0.60768709786657638,\n",
       " 0.35500000000000009: 0.60667118184896718,\n",
       " 0.35550000000000009: 0.6063325431764307,\n",
       " 0.35600000000000009: 0.60531662715882151,\n",
       " 0.35650000000000009: 0.60531662715882151,\n",
       " 0.3570000000000001: 0.60531662715882151,\n",
       " 0.3575000000000001: 0.60430071114121231,\n",
       " 0.3580000000000001: 0.60345411445987129,\n",
       " 0.3585000000000001: 0.60379275313240777,\n",
       " 0.3590000000000001: 0.60362343379613947,\n",
       " 0.3595000000000001: 0.60430071114121231,\n",
       " 0.3600000000000001: 0.60362343379613947,\n",
       " 0.3605000000000001: 0.60311547578733493,\n",
       " 0.3610000000000001: 0.60108364375211654,\n",
       " 0.3615000000000001: 0.59905181171689803,\n",
       " 0.3620000000000001: 0.59786657636302065,\n",
       " 0.3625000000000001: 0.59718929901794782,\n",
       " 0.3630000000000001: 0.59685066034541145,\n",
       " 0.3635000000000001: 0.59718929901794782,\n",
       " 0.3640000000000001: 0.5963427023366068,\n",
       " 0.3645000000000001: 0.59549610565526578,\n",
       " 0.3650000000000001: 0.59448018963765659,\n",
       " 0.3655000000000001: 0.59431087030138841,\n",
       " 0.3660000000000001: 0.59363359295631557,\n",
       " 0.3665000000000001: 0.59177108025736536,\n",
       " 0.3670000000000001: 0.59007788689468332,\n",
       " 0.3675000000000001: 0.58906197087707413,\n",
       " 0.3680000000000001: 0.58821537419573311,\n",
       " 0.36850000000000011: 0.58686081950558755,\n",
       " 0.36900000000000011: 0.58601422282424653,\n",
       " 0.36950000000000011: 0.58499830680663734,\n",
       " 0.37000000000000011: 0.5843210294615645,\n",
       " 0.37050000000000011: 0.58398239078902814,\n",
       " 0.37100000000000011: 0.58347443278022348,\n",
       " 0.37150000000000011: 0.58279715543515065,\n",
       " 0.37200000000000011: 0.58211987809007792,\n",
       " 0.37250000000000011: 0.58127328140873691,\n",
       " 0.37300000000000011: 0.57991872671859124,\n",
       " 0.37350000000000011: 0.57873349136471386,\n",
       " 0.37400000000000011: 0.57771757534710466,\n",
       " 0.37450000000000011: 0.57602438198442263,\n",
       " 0.37500000000000011: 0.57517778530308161,\n",
       " 0.37550000000000011: 0.57466982729427696,\n",
       " 0.37600000000000011: 0.57450050795800878,\n",
       " 0.37650000000000011: 0.57297663393159504,\n",
       " 0.37700000000000011: 0.57246867592279038,\n",
       " 0.37750000000000011: 0.57060616322384017,\n",
       " 0.37800000000000011: 0.56975956654249915,\n",
       " 0.37850000000000011: 0.56874365052488995,\n",
       " 0.37900000000000011: 0.56840501185235359,\n",
       " 0.37950000000000012: 0.5682356925160853}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for threshold in np.arange(0.28, 0.38, step=0.0005):\n",
    "    prediction = train['min_max_similarity'].apply(lambda x: x < threshold)\n",
    "    results[threshold] = (train['different_author'] == prediction).mean()\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_threshold = 0.3475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60891938250428812"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = val['min_max_similarity'].apply(lambda x: x < opt_threshold)\n",
    "(val['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different_author\n",
      "False    0.636205\n",
      "True     0.620226\n",
      "Name: cos_similarity, dtype: float64\n",
      "different_author\n",
      "False    0.633924\n",
      "True     0.622803\n",
      "Name: cos_similarity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train.groupby('different_author').mean()['cos_similarity'])\n",
    "print(val.groupby('different_author').mean()['cos_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.59999999999999998: 0.53081611920081273,\n",
       " 0.60049999999999992: 0.53166271588215375,\n",
       " 0.60099999999999987: 0.53098543853708091,\n",
       " 0.60149999999999981: 0.53200135455469011,\n",
       " 0.60199999999999976: 0.5321706738909584,\n",
       " 0.6024999999999997: 0.53250931256349476,\n",
       " 0.60299999999999965: 0.53403318658990862,\n",
       " 0.60349999999999959: 0.53403318658990862,\n",
       " 0.60399999999999954: 0.53521842194378599,\n",
       " 0.60449999999999948: 0.53640365729766337,\n",
       " 0.60499999999999943: 0.53674229597019985,\n",
       " 0.60549999999999937: 0.53725025397900439,\n",
       " 0.60599999999999932: 0.53741957331527257,\n",
       " 0.60649999999999926: 0.53741957331527257,\n",
       " 0.60699999999999921: 0.53843548933288177,\n",
       " 0.60749999999999915: 0.53995936335929562,\n",
       " 0.6079999999999991: 0.54063664070436845,\n",
       " 0.60849999999999904: 0.54097527937690482,\n",
       " 0.60899999999999899: 0.54232983406705049,\n",
       " 0.60949999999999893: 0.54402302742973252,\n",
       " 0.60999999999999888: 0.54351506942092787,\n",
       " 0.61049999999999882: 0.54368438875719605,\n",
       " 0.61099999999999877: 0.54470030477480524,\n",
       " 0.61149999999999871: 0.54537758211987808,\n",
       " 0.61199999999999866: 0.5452082627836099,\n",
       " 0.6124999999999986: 0.54656281747375546,\n",
       " 0.61299999999999855: 0.54740941415509647,\n",
       " 0.61349999999999849: 0.54724009481882829,\n",
       " 0.61399999999999844: 0.54757873349136477,\n",
       " 0.61449999999999838: 0.54791737216390113,\n",
       " 0.61499999999999833: 0.54961056552658316,\n",
       " 0.61549999999999827: 0.54994920419911952,\n",
       " 0.61599999999999822: 0.55181171689806974,\n",
       " 0.61649999999999816: 0.55282763291567893,\n",
       " 0.61699999999999811: 0.55418218760582461,\n",
       " 0.61749999999999805: 0.55435150694209279,\n",
       " 0.617999999999998: 0.55452082627836097,\n",
       " 0.61849999999999794: 0.55384354893328813,\n",
       " 0.61899999999999789: 0.5551981036234338,\n",
       " 0.61949999999999783: 0.55587538096850664,\n",
       " 0.61999999999999778: 0.55638333897731118,\n",
       " 0.62049999999999772: 0.55706061632238402,\n",
       " 0.62099999999999766: 0.55841517101252958,\n",
       " 0.62149999999999761: 0.55773789366745685,\n",
       " 0.62199999999999755: 0.55790721300372503,\n",
       " 0.6224999999999975: 0.55773789366745685,\n",
       " 0.62299999999999744: 0.55807653233999321,\n",
       " 0.62349999999999739: 0.55892312902133423,\n",
       " 0.62399999999999733: 0.5582458516762614,\n",
       " 0.62449999999999728: 0.55892312902133423,\n",
       " 0.62499999999999722: 0.55943108703013888,\n",
       " 0.62549999999999717: 0.55976972570267525,\n",
       " 0.62599999999999711: 0.56010836437521161,\n",
       " 0.62649999999999706: 0.56197087707416182,\n",
       " 0.626999999999997: 0.56383338977311204,\n",
       " 0.62749999999999695: 0.56434134778191669,\n",
       " 0.62799999999999689: 0.56569590247206236,\n",
       " 0.62849999999999684: 0.56620386048086691,\n",
       " 0.62899999999999678: 0.56603454114459872,\n",
       " 0.62949999999999673: 0.56688113782593974,\n",
       " 0.62999999999999667: 0.5672197764984761,\n",
       " 0.63049999999999662: 0.56874365052488995,\n",
       " 0.63099999999999656: 0.56857433118862177,\n",
       " 0.63149999999999651: 0.56755841517101258,\n",
       " 0.63199999999999645: 0.56705045716220792,\n",
       " 0.6324999999999964: 0.5672197764984761,\n",
       " 0.63299999999999634: 0.56772773450728076,\n",
       " 0.63349999999999629: 0.56840501185235359,\n",
       " 0.63399999999999623: 0.56806637317981712,\n",
       " 0.63449999999999618: 0.5682356925160853,\n",
       " 0.63499999999999612: 0.56874365052488995,\n",
       " 0.63549999999999607: 0.56959024720623097,\n",
       " 0.63599999999999601: 0.57043684388757199,\n",
       " 0.63649999999999596: 0.57043684388757199,\n",
       " 0.6369999999999959: 0.56975956654249915,\n",
       " 0.63749999999999585: 0.571283440568913,\n",
       " 0.63799999999999579: 0.57111412123264482,\n",
       " 0.63849999999999574: 0.571283440568913,\n",
       " 0.63899999999999568: 0.57297663393159504,\n",
       " 0.63949999999999563: 0.57213003725025402,\n",
       " 0.63999999999999557: 0.57213003725025402,\n",
       " 0.64049999999999552: 0.57382323061293605,\n",
       " 0.64099999999999546: 0.57382323061293605,\n",
       " 0.64149999999999541: 0.5733152726041314,\n",
       " 0.64199999999999535: 0.57280731459532674,\n",
       " 0.6424999999999953: 0.57382323061293605,\n",
       " 0.64299999999999524: 0.57365391127666776,\n",
       " 0.64349999999999519: 0.57365391127666776,\n",
       " 0.64399999999999513: 0.57450050795800878,\n",
       " 0.64449999999999508: 0.57500846596681343,\n",
       " 0.64499999999999502: 0.57466982729427696,\n",
       " 0.64549999999999497: 0.57416186928547241,\n",
       " 0.64599999999999491: 0.57466982729427696,\n",
       " 0.64649999999999486: 0.57585506264815445,\n",
       " 0.6469999999999948: 0.57636302065695899,\n",
       " 0.64749999999999475: 0.57602438198442263,\n",
       " 0.64799999999999469: 0.57619370132069081,\n",
       " 0.64849999999999464: 0.57585506264815445,\n",
       " 0.64899999999999458: 0.57653233999322717,\n",
       " 0.64949999999999453: 0.57534710463934979,\n",
       " 0.64999999999999447: 0.57602438198442263,\n",
       " 0.65049999999999442: 0.57602438198442263,\n",
       " 0.65099999999999436: 0.57771757534710466,\n",
       " 0.65149999999999431: 0.57771757534710466,\n",
       " 0.65199999999999425: 0.57771757534710466,\n",
       " 0.6524999999999942: 0.57737893667456819,\n",
       " 0.65299999999999414: 0.57720961733830001,\n",
       " 0.65349999999999409: 0.57687097866576365,\n",
       " 0.65399999999999403: 0.57687097866576365,\n",
       " 0.65449999999999398: 0.57670165932949546,\n",
       " 0.65499999999999392: 0.57602438198442263,\n",
       " 0.65549999999999387: 0.57670165932949546,\n",
       " 0.65599999999999381: 0.57568574331188627,\n",
       " 0.65649999999999376: 0.57619370132069081,\n",
       " 0.6569999999999937: 0.57653233999322717,\n",
       " 0.65749999999999365: 0.57585506264815445,\n",
       " 0.65799999999999359: 0.57653233999322717,\n",
       " 0.65849999999999353: 0.57636302065695899,\n",
       " 0.65899999999999348: 0.57704029800203183,\n",
       " 0.65949999999999342: 0.57687097866576365,\n",
       " 0.65999999999999337: 0.57636302065695899,\n",
       " 0.66049999999999331: 0.57636302065695899,\n",
       " 0.66099999999999326: 0.57585506264815445,\n",
       " 0.6614999999999932: 0.57483914663054525,\n",
       " 0.66199999999999315: 0.57399254994920423,\n",
       " 0.66249999999999309: 0.57466982729427696,\n",
       " 0.66299999999999304: 0.57483914663054525,\n",
       " 0.66349999999999298: 0.57483914663054525,\n",
       " 0.66399999999999293: 0.5743311886217406,\n",
       " 0.66449999999999287: 0.57619370132069081,\n",
       " 0.66499999999999282: 0.57636302065695899,\n",
       " 0.66549999999999276: 0.57619370132069081,\n",
       " 0.66599999999999271: 0.57619370132069081,\n",
       " 0.66649999999999265: 0.57568574331188627,\n",
       " 0.6669999999999926: 0.57551642397561797,\n",
       " 0.66749999999999254: 0.57636302065695899,\n",
       " 0.66799999999999249: 0.57754825601083648,\n",
       " 0.66849999999999243: 0.57704029800203183,\n",
       " 0.66899999999999238: 0.57704029800203183,\n",
       " 0.66949999999999232: 0.57805621401964102,\n",
       " 0.66999999999999227: 0.57856417202844568,\n",
       " 0.67049999999999221: 0.57737893667456819,\n",
       " 0.67099999999999216: 0.57771757534710466,\n",
       " 0.6714999999999921: 0.57704029800203183,\n",
       " 0.67199999999999205: 0.57720961733830001,\n",
       " 0.67249999999999199: 0.57704029800203183,\n",
       " 0.67299999999999194: 0.57653233999322717,\n",
       " 0.67349999999999188: 0.57619370132069081,\n",
       " 0.67399999999999183: 0.57636302065695899,\n",
       " 0.67449999999999177: 0.57568574331188627,\n",
       " 0.67499999999999172: 0.57534710463934979,\n",
       " 0.67549999999999166: 0.57517778530308161,\n",
       " 0.67599999999999161: 0.57500846596681343,\n",
       " 0.67649999999999155: 0.5743311886217406,\n",
       " 0.6769999999999915: 0.57483914663054525,\n",
       " 0.67749999999999144: 0.57466982729427696,\n",
       " 0.67799999999999139: 0.57382323061293605,\n",
       " 0.67849999999999133: 0.57297663393159504,\n",
       " 0.67899999999999128: 0.57365391127666776,\n",
       " 0.67949999999999122: 0.57280731459532674,\n",
       " 0.67999999999999117: 0.57280731459532674,\n",
       " 0.68049999999999111: 0.57297663393159504,\n",
       " 0.68099999999999106: 0.5722993565865222,\n",
       " 0.681499999999991: 0.57246867592279038,\n",
       " 0.68199999999999095: 0.57213003725025402,\n",
       " 0.68249999999999089: 0.57162207924144937,\n",
       " 0.68299999999999084: 0.57162207924144937,\n",
       " 0.68349999999999078: 0.57246867592279038,\n",
       " 0.68399999999999073: 0.57145275990518118,\n",
       " 0.68449999999999067: 0.57094480189637653,\n",
       " 0.68499999999999062: 0.57094480189637653,\n",
       " 0.68549999999999056: 0.57026752455130381,\n",
       " 0.68599999999999051: 0.57043684388757199,\n",
       " 0.68649999999999045: 0.57043684388757199,\n",
       " 0.6869999999999904: 0.57060616322384017,\n",
       " 0.68749999999999034: 0.57026752455130381,\n",
       " 0.68799999999999029: 0.56975956654249915,\n",
       " 0.68849999999999023: 0.5692516085336945,\n",
       " 0.68899999999999018: 0.56789705384354894,\n",
       " 0.68949999999999012: 0.56738909583474428,\n",
       " 0.68999999999999007: 0.56654249915340327,\n",
       " 0.69049999999999001: 0.56586522180833054,\n",
       " 0.69099999999998996: 0.56434134778191669,\n",
       " 0.6914999999999899: 0.56400270910938033,\n",
       " 0.69199999999998985: 0.56383338977311204,\n",
       " 0.69249999999998979: 0.56298679309177113,\n",
       " 0.69299999999998974: 0.56298679309177113,\n",
       " 0.69349999999998968: 0.56247883508296648,\n",
       " 0.69399999999998963: 0.56163223840162546,\n",
       " 0.69449999999998957: 0.56044700304774808,\n",
       " 0.69499999999998952: 0.55875380968506605,\n",
       " 0.69549999999998946: 0.55892312902133423,\n",
       " 0.6959999999999894: 0.55858449034879787,\n",
       " 0.69649999999998935: 0.55875380968506605,\n",
       " 0.69699999999998929: 0.55790721300372503,\n",
       " 0.69749999999998924: 0.55706061632238402,\n",
       " 0.69799999999998918: 0.55655265831357936,\n",
       " 0.69849999999998913: 0.55604470030477482,\n",
       " 0.69899999999998907: 0.55485946495089744,\n",
       " 0.69949999999998902: 0.55452082627836097}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_results = {}\n",
    "\n",
    "for threshold in np.arange(0.6, 0.7, step=0.0005):\n",
    "    prediction = train['cos_similarity'].apply(lambda x: x < threshold)\n",
    "    cos_results[threshold] = (train['different_author'] == prediction).mean()\n",
    "\n",
    "cos_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57255574614065186"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_cos_threshold = 0.66\n",
    "\n",
    "prediction = val['cos_similarity'].apply(lambda x: x < opt_cos_threshold)\n",
    "(val['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler_minmax_sim = MinMaxScaler().fit(train['min_max_similarity'].values.reshape(-1, 1))\n",
    "\n",
    "train['min_max_similarity_scaled'] = scaler_minmax_sim.transform(train['min_max_similarity'].values.reshape(-1, 1))\n",
    "val['min_max_similarity_scaled'] = scaler_minmax_sim.transform(val['min_max_similarity'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler_cos_sim = MinMaxScaler().fit(train['cos_similarity'].values.reshape(-1, 1))\n",
    "\n",
    "train['cos_similarity_scaled'] = scaler_cos_sim.transform(train['cos_similarity'].values.reshape(-1, 1))\n",
    "val['cos_similarity_scaled'] = scaler_cos_sim.transform(val['cos_similarity'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_0</th>\n",
       "      <th>A_1</th>\n",
       "      <th>A_2</th>\n",
       "      <th>A_3</th>\n",
       "      <th>A_4</th>\n",
       "      <th>A_5</th>\n",
       "      <th>A_6</th>\n",
       "      <th>A_7</th>\n",
       "      <th>A_8</th>\n",
       "      <th>A_9</th>\n",
       "      <th>...</th>\n",
       "      <th>B_828</th>\n",
       "      <th>B_829</th>\n",
       "      <th>B_830</th>\n",
       "      <th>B_831</th>\n",
       "      <th>B_832</th>\n",
       "      <th>different_author</th>\n",
       "      <th>min_max_similarity</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>min_max_similarity_scaled</th>\n",
       "      <th>cos_similarity_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.591167</td>\n",
       "      <td>0.238369</td>\n",
       "      <td>0.393659</td>\n",
       "      <td>0.654621</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.544193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.470222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072784</td>\n",
       "      <td>True</td>\n",
       "      <td>0.341655</td>\n",
       "      <td>0.670623</td>\n",
       "      <td>0.639588</td>\n",
       "      <td>0.716054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500083</td>\n",
       "      <td>0.186608</td>\n",
       "      <td>0.325804</td>\n",
       "      <td>0.428397</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.448047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409647</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.339214</td>\n",
       "      <td>0.668914</td>\n",
       "      <td>0.633601</td>\n",
       "      <td>0.713424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.589960</td>\n",
       "      <td>0.255750</td>\n",
       "      <td>0.393005</td>\n",
       "      <td>0.653535</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.552540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.319101</td>\n",
       "      <td>0.626929</td>\n",
       "      <td>0.584265</td>\n",
       "      <td>0.648829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.519795</td>\n",
       "      <td>0.224352</td>\n",
       "      <td>0.295311</td>\n",
       "      <td>0.506321</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.553020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.366793</td>\n",
       "      <td>0.708512</td>\n",
       "      <td>0.701247</td>\n",
       "      <td>0.774349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.616025</td>\n",
       "      <td>0.304250</td>\n",
       "      <td>0.248293</td>\n",
       "      <td>0.639090</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.629050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.417508</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.416457</td>\n",
       "      <td>0.773555</td>\n",
       "      <td>0.823062</td>\n",
       "      <td>0.874422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1671 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        A_0       A_1       A_2       A_3   A_4       A_5  A_6  A_7       A_8  \\\n",
       "0  0.591167  0.238369  0.393659  0.654621  0.23  0.544193  0.0  0.0  0.470222   \n",
       "1  0.500083  0.186608  0.325804  0.428397  0.00  0.448047  0.0  0.0  0.409647   \n",
       "2  0.589960  0.255750  0.393005  0.653535  0.23  0.552540  0.0  0.0  0.469346   \n",
       "3  0.519795  0.224352  0.295311  0.506321  0.00  0.553020  0.0  0.0  0.482341   \n",
       "4  0.616025  0.304250  0.248293  0.639090  0.00  0.629050  0.0  0.0  0.417508   \n",
       "\n",
       "        A_9          ...               B_828  B_829  B_830  B_831     B_832  \\\n",
       "0  0.000000          ...            0.095192    0.0    0.0    0.0  0.072784   \n",
       "1  0.069231          ...            0.742500    0.0    0.0    0.0  0.000000   \n",
       "2  0.000000          ...            0.000000    0.0    0.0    0.0  0.000000   \n",
       "3  0.000000          ...            0.247191    0.0    0.0    0.0  0.000000   \n",
       "4  0.134831          ...            0.081481    0.0    0.0    0.0  0.000000   \n",
       "\n",
       "   different_author  min_max_similarity  cos_similarity  \\\n",
       "0              True            0.341655        0.670623   \n",
       "1              True            0.339214        0.668914   \n",
       "2              True            0.319101        0.626929   \n",
       "3             False            0.366793        0.708512   \n",
       "4             False            0.416457        0.773555   \n",
       "\n",
       "   min_max_similarity_scaled  cos_similarity_scaled  \n",
       "0                   0.639588               0.716054  \n",
       "1                   0.633601               0.713424  \n",
       "2                   0.584265               0.648829  \n",
       "3                   0.701247               0.774349  \n",
       "4                   0.823062               0.874422  \n",
       "\n",
       "[5 rows x 1671 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_0</th>\n",
       "      <th>A_1</th>\n",
       "      <th>A_2</th>\n",
       "      <th>A_3</th>\n",
       "      <th>A_4</th>\n",
       "      <th>A_5</th>\n",
       "      <th>A_6</th>\n",
       "      <th>A_7</th>\n",
       "      <th>A_8</th>\n",
       "      <th>A_9</th>\n",
       "      <th>...</th>\n",
       "      <th>B_828</th>\n",
       "      <th>B_829</th>\n",
       "      <th>B_830</th>\n",
       "      <th>B_831</th>\n",
       "      <th>B_832</th>\n",
       "      <th>different_author</th>\n",
       "      <th>min_max_similarity</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>min_max_similarity_scaled</th>\n",
       "      <th>cos_similarity_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.530822</td>\n",
       "      <td>0.224139</td>\n",
       "      <td>0.130640</td>\n",
       "      <td>0.556357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.394825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.426404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.319091</td>\n",
       "      <td>0.652089</td>\n",
       "      <td>0.584242</td>\n",
       "      <td>0.687538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550263</td>\n",
       "      <td>0.219075</td>\n",
       "      <td>0.114079</td>\n",
       "      <td>0.433775</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.390259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.410330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.331378</td>\n",
       "      <td>0.646179</td>\n",
       "      <td>0.614380</td>\n",
       "      <td>0.678446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647249</td>\n",
       "      <td>0.272249</td>\n",
       "      <td>0.184150</td>\n",
       "      <td>0.398342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.364741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.329187</td>\n",
       "      <td>0.632442</td>\n",
       "      <td>0.609007</td>\n",
       "      <td>0.657311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440422</td>\n",
       "      <td>0.268487</td>\n",
       "      <td>0.306748</td>\n",
       "      <td>0.674705</td>\n",
       "      <td>0.205357</td>\n",
       "      <td>0.524594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.329540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.321452</td>\n",
       "      <td>0.629388</td>\n",
       "      <td>0.590034</td>\n",
       "      <td>0.652612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.472994</td>\n",
       "      <td>0.267232</td>\n",
       "      <td>0.273820</td>\n",
       "      <td>0.416469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333545</td>\n",
       "      <td>0.657252</td>\n",
       "      <td>0.619694</td>\n",
       "      <td>0.695483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1671 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        A_0       A_1       A_2       A_3       A_4       A_5  A_6  A_7  \\\n",
       "0  0.530822  0.224139  0.130640  0.556357  0.000000  0.394825  0.0  0.0   \n",
       "1  0.550263  0.219075  0.114079  0.433775  0.232323  0.390259  0.0  0.0   \n",
       "2  0.647249  0.272249  0.184150  0.398342  0.000000  0.334283  0.0  0.0   \n",
       "3  0.440422  0.268487  0.306748  0.674705  0.205357  0.524594  0.0  0.0   \n",
       "4  0.472994  0.267232  0.273820  0.416469  0.000000  0.451413  0.0  0.0   \n",
       "\n",
       "        A_8  A_9          ...               B_828  B_829  B_830  B_831  B_832  \\\n",
       "0  0.426404  0.0          ...            0.166667    0.0    0.0    0.0    0.0   \n",
       "1  0.410330  0.0          ...            0.119855    0.0    0.0    0.0    0.0   \n",
       "2  0.364741  0.0          ...            0.147321    0.0    0.0    0.0    0.0   \n",
       "3  0.329540  0.0          ...            0.000000    0.0    0.0    0.0    0.0   \n",
       "4  0.551084  0.0          ...            0.095376    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   different_author  min_max_similarity  cos_similarity  \\\n",
       "0              True            0.319091        0.652089   \n",
       "1              True            0.331378        0.646179   \n",
       "2             False            0.329187        0.632442   \n",
       "3             False            0.321452        0.629388   \n",
       "4              True            0.333545        0.657252   \n",
       "\n",
       "   min_max_similarity_scaled  cos_similarity_scaled  \n",
       "0                   0.584242               0.687538  \n",
       "1                   0.614380               0.678446  \n",
       "2                   0.609007               0.657311  \n",
       "3                   0.590034               0.652612  \n",
       "4                   0.619694               0.695483  \n",
       "\n",
       "[5 rows x 1671 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['avg_similarity_scaled'] = train.apply(lambda x: (x.min_max_similarity_scaled + x.cos_similarity_scaled) / 2, axis=1)\n",
    "val['avg_similarity_scaled'] = val.apply(lambda x: (x.min_max_similarity_scaled + x.cos_similarity_scaled) / 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_0</th>\n",
       "      <th>A_1</th>\n",
       "      <th>A_2</th>\n",
       "      <th>A_3</th>\n",
       "      <th>A_4</th>\n",
       "      <th>A_5</th>\n",
       "      <th>A_6</th>\n",
       "      <th>A_7</th>\n",
       "      <th>A_8</th>\n",
       "      <th>A_9</th>\n",
       "      <th>...</th>\n",
       "      <th>B_829</th>\n",
       "      <th>B_830</th>\n",
       "      <th>B_831</th>\n",
       "      <th>B_832</th>\n",
       "      <th>different_author</th>\n",
       "      <th>min_max_similarity</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>min_max_similarity_scaled</th>\n",
       "      <th>cos_similarity_scaled</th>\n",
       "      <th>avg_similarity_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.591167</td>\n",
       "      <td>0.238369</td>\n",
       "      <td>0.393659</td>\n",
       "      <td>0.654621</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.544193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.470222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072784</td>\n",
       "      <td>True</td>\n",
       "      <td>0.341655</td>\n",
       "      <td>0.670623</td>\n",
       "      <td>0.639588</td>\n",
       "      <td>0.716054</td>\n",
       "      <td>0.677821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500083</td>\n",
       "      <td>0.186608</td>\n",
       "      <td>0.325804</td>\n",
       "      <td>0.428397</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.448047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409647</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.339214</td>\n",
       "      <td>0.668914</td>\n",
       "      <td>0.633601</td>\n",
       "      <td>0.713424</td>\n",
       "      <td>0.673512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.589960</td>\n",
       "      <td>0.255750</td>\n",
       "      <td>0.393005</td>\n",
       "      <td>0.653535</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.552540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.319101</td>\n",
       "      <td>0.626929</td>\n",
       "      <td>0.584265</td>\n",
       "      <td>0.648829</td>\n",
       "      <td>0.616547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.519795</td>\n",
       "      <td>0.224352</td>\n",
       "      <td>0.295311</td>\n",
       "      <td>0.506321</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.553020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.366793</td>\n",
       "      <td>0.708512</td>\n",
       "      <td>0.701247</td>\n",
       "      <td>0.774349</td>\n",
       "      <td>0.737798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.616025</td>\n",
       "      <td>0.304250</td>\n",
       "      <td>0.248293</td>\n",
       "      <td>0.639090</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.629050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.417508</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.416457</td>\n",
       "      <td>0.773555</td>\n",
       "      <td>0.823062</td>\n",
       "      <td>0.874422</td>\n",
       "      <td>0.848742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1672 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        A_0       A_1       A_2       A_3   A_4       A_5  A_6  A_7       A_8  \\\n",
       "0  0.591167  0.238369  0.393659  0.654621  0.23  0.544193  0.0  0.0  0.470222   \n",
       "1  0.500083  0.186608  0.325804  0.428397  0.00  0.448047  0.0  0.0  0.409647   \n",
       "2  0.589960  0.255750  0.393005  0.653535  0.23  0.552540  0.0  0.0  0.469346   \n",
       "3  0.519795  0.224352  0.295311  0.506321  0.00  0.553020  0.0  0.0  0.482341   \n",
       "4  0.616025  0.304250  0.248293  0.639090  0.00  0.629050  0.0  0.0  0.417508   \n",
       "\n",
       "        A_9          ...            B_829  B_830  B_831     B_832  \\\n",
       "0  0.000000          ...              0.0    0.0    0.0  0.072784   \n",
       "1  0.069231          ...              0.0    0.0    0.0  0.000000   \n",
       "2  0.000000          ...              0.0    0.0    0.0  0.000000   \n",
       "3  0.000000          ...              0.0    0.0    0.0  0.000000   \n",
       "4  0.134831          ...              0.0    0.0    0.0  0.000000   \n",
       "\n",
       "   different_author  min_max_similarity  cos_similarity  \\\n",
       "0              True            0.341655        0.670623   \n",
       "1              True            0.339214        0.668914   \n",
       "2              True            0.319101        0.626929   \n",
       "3             False            0.366793        0.708512   \n",
       "4             False            0.416457        0.773555   \n",
       "\n",
       "   min_max_similarity_scaled  cos_similarity_scaled  avg_similarity_scaled  \n",
       "0                   0.639588               0.716054               0.677821  \n",
       "1                   0.633601               0.713424               0.673512  \n",
       "2                   0.584265               0.648829               0.616547  \n",
       "3                   0.701247               0.774349               0.737798  \n",
       "4                   0.823062               0.874422               0.848742  \n",
       "\n",
       "[5 rows x 1672 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different_author\n",
      "False    0.647932\n",
      "True     0.611883\n",
      "Name: avg_similarity_scaled, dtype: float64\n",
      "different_author\n",
      "False    0.644889\n",
      "True     0.615914\n",
      "Name: avg_similarity_scaled, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train.groupby('different_author').mean()['avg_similarity_scaled'])\n",
    "print(val.groupby('different_author').mean()['avg_similarity_scaled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.59999999999999998: 0.56738909583474428,\n",
       " 0.60049999999999992: 0.56789705384354894,\n",
       " 0.60099999999999987: 0.56806637317981712,\n",
       " 0.60149999999999981: 0.56959024720623097,\n",
       " 0.60199999999999976: 0.57094480189637653,\n",
       " 0.6024999999999997: 0.57162207924144937,\n",
       " 0.60299999999999965: 0.57246867592279038,\n",
       " 0.60349999999999959: 0.57246867592279038,\n",
       " 0.60399999999999954: 0.57314595326786322,\n",
       " 0.60449999999999948: 0.57246867592279038,\n",
       " 0.60499999999999943: 0.57314595326786322,\n",
       " 0.60549999999999937: 0.57382323061293605,\n",
       " 0.60599999999999932: 0.57348459194039958,\n",
       " 0.60649999999999926: 0.57399254994920423,\n",
       " 0.60699999999999921: 0.57517778530308161,\n",
       " 0.60749999999999915: 0.57568574331188627,\n",
       " 0.6079999999999991: 0.57653233999322717,\n",
       " 0.60849999999999904: 0.57619370132069081,\n",
       " 0.60899999999999899: 0.57602438198442263,\n",
       " 0.60949999999999893: 0.57534710463934979,\n",
       " 0.60999999999999888: 0.57568574331188627,\n",
       " 0.61049999999999882: 0.57534710463934979,\n",
       " 0.61099999999999877: 0.57517778530308161,\n",
       " 0.61149999999999871: 0.57568574331188627,\n",
       " 0.61199999999999866: 0.57534710463934979,\n",
       " 0.6124999999999986: 0.57500846596681343,\n",
       " 0.61299999999999855: 0.57500846596681343,\n",
       " 0.61349999999999849: 0.57500846596681343,\n",
       " 0.61399999999999844: 0.57551642397561797,\n",
       " 0.61449999999999838: 0.57534710463934979,\n",
       " 0.61499999999999833: 0.57551642397561797,\n",
       " 0.61549999999999827: 0.57551642397561797,\n",
       " 0.61599999999999822: 0.57551642397561797,\n",
       " 0.61649999999999816: 0.57500846596681343,\n",
       " 0.61699999999999811: 0.57517778530308161,\n",
       " 0.61749999999999805: 0.57619370132069081,\n",
       " 0.617999999999998: 0.57653233999322717,\n",
       " 0.61849999999999794: 0.57670165932949546,\n",
       " 0.61899999999999789: 0.57720961733830001,\n",
       " 0.61949999999999783: 0.57737893667456819,\n",
       " 0.61999999999999778: 0.57754825601083648,\n",
       " 0.62049999999999772: 0.57788689468337284,\n",
       " 0.62099999999999766: 0.57771757534710466,\n",
       " 0.62149999999999761: 0.57737893667456819,\n",
       " 0.62199999999999755: 0.57805621401964102,\n",
       " 0.6224999999999975: 0.5783948526921775,\n",
       " 0.62299999999999744: 0.5792414493735184,\n",
       " 0.62349999999999739: 0.57941076870978669,\n",
       " 0.62399999999999733: 0.57958008804605488,\n",
       " 0.62449999999999728: 0.57974940738232306,\n",
       " 0.62499999999999722: 0.58161192008127327,\n",
       " 0.62549999999999717: 0.58161192008127327,\n",
       " 0.62599999999999711: 0.58161192008127327,\n",
       " 0.62649999999999706: 0.58228919742634611,\n",
       " 0.626999999999997: 0.58296647477141894,\n",
       " 0.62749999999999695: 0.58279715543515065,\n",
       " 0.62799999999999689: 0.58313579410768712,\n",
       " 0.62849999999999684: 0.58279715543515065,\n",
       " 0.62899999999999678: 0.58262783609888247,\n",
       " 0.62949999999999673: 0.58279715543515065,\n",
       " 0.62999999999999667: 0.58296647477141894,\n",
       " 0.63049999999999662: 0.58347443278022348,\n",
       " 0.63099999999999656: 0.58550626481544188,\n",
       " 0.63149999999999651: 0.58516762614290552,\n",
       " 0.63199999999999645: 0.58550626481544188,\n",
       " 0.6324999999999964: 0.58567558415171017,\n",
       " 0.63299999999999634: 0.58550626481544188,\n",
       " 0.63349999999999629: 0.58618354216051471,\n",
       " 0.63399999999999623: 0.58753809685066039,\n",
       " 0.63449999999999618: 0.58872333220453776,\n",
       " 0.63499999999999612: 0.58906197087707413,\n",
       " 0.63549999999999607: 0.59109380291229263,\n",
       " 0.63599999999999601: 0.59143244158482899,\n",
       " 0.63649999999999596: 0.59126312224856081,\n",
       " 0.6369999999999959: 0.59075516423975616,\n",
       " 0.63749999999999585: 0.59126312224856081,\n",
       " 0.63799999999999579: 0.59058584490348798,\n",
       " 0.63849999999999574: 0.58990856755841514,\n",
       " 0.63899999999999568: 0.5904165255672198,\n",
       " 0.63949999999999563: 0.59058584490348798,\n",
       " 0.63999999999999557: 0.59024720623095162,\n",
       " 0.64049999999999552: 0.59143244158482899,\n",
       " 0.64099999999999546: 0.59210971892990183,\n",
       " 0.64149999999999541: 0.59194039959363365,\n",
       " 0.64199999999999535: 0.59295631561124285,\n",
       " 0.6424999999999953: 0.59244835760243819,\n",
       " 0.64299999999999524: 0.59210971892990183,\n",
       " 0.64349999999999519: 0.59261767693870637,\n",
       " 0.64399999999999513: 0.59160176092109718,\n",
       " 0.64449999999999508: 0.59160176092109718,\n",
       " 0.64499999999999502: 0.59143244158482899,\n",
       " 0.64549999999999497: 0.59160176092109718,\n",
       " 0.64599999999999491: 0.59295631561124285,\n",
       " 0.64649999999999486: 0.59363359295631557,\n",
       " 0.6469999999999948: 0.59397223162885204,\n",
       " 0.64749999999999475: 0.59363359295631557,\n",
       " 0.64799999999999469: 0.59448018963765659,\n",
       " 0.64849999999999464: 0.59431087030138841,\n",
       " 0.64899999999999458: 0.59397223162885204,\n",
       " 0.64949999999999453: 0.59481882831019306,\n",
       " 0.64999999999999447: 0.59549610565526578,\n",
       " 0.65049999999999442: 0.5963427023366068,\n",
       " 0.65099999999999436: 0.597358618354216,\n",
       " 0.65149999999999431: 0.59685066034541145,\n",
       " 0.65199999999999425: 0.59718929901794782,\n",
       " 0.6524999999999942: 0.59718929901794782,\n",
       " 0.65299999999999414: 0.59685066034541145,\n",
       " 0.65349999999999409: 0.597358618354216,\n",
       " 0.65399999999999403: 0.597358618354216,\n",
       " 0.65449999999999398: 0.59854385370809349,\n",
       " 0.65499999999999392: 0.59837453437182531,\n",
       " 0.65549999999999387: 0.59718929901794782,\n",
       " 0.65599999999999381: 0.59701997968167964,\n",
       " 0.65649999999999376: 0.597358618354216,\n",
       " 0.6569999999999937: 0.59803589569928883,\n",
       " 0.65749999999999365: 0.59803589569928883,\n",
       " 0.65799999999999359: 0.59854385370809349,\n",
       " 0.65849999999999353: 0.59854385370809349,\n",
       " 0.65899999999999348: 0.59837453437182531,\n",
       " 0.65949999999999342: 0.59854385370809349,\n",
       " 0.65999999999999337: 0.5993904503894345,\n",
       " 0.66049999999999331: 0.59905181171689803,\n",
       " 0.66099999999999326: 0.59922113105316632,\n",
       " 0.6614999999999932: 0.59955976972570268,\n",
       " 0.66199999999999315: 0.59989840839823905,\n",
       " 0.66249999999999309: 0.60006772773450723,\n",
       " 0.66299999999999304: 0.60091432441584824,\n",
       " 0.66349999999999298: 0.60006772773450723,\n",
       " 0.66399999999999293: 0.59989840839823905,\n",
       " 0.66449999999999287: 0.59922113105316632,\n",
       " 0.66499999999999282: 0.59922113105316632,\n",
       " 0.66549999999999276: 0.59922113105316632,\n",
       " 0.66599999999999271: 0.5993904503894345,\n",
       " 0.66649999999999265: 0.59989840839823905,\n",
       " 0.6669999999999926: 0.6014222824246529,\n",
       " 0.66749999999999254: 0.6014222824246529,\n",
       " 0.66799999999999249: 0.60176092109718926,\n",
       " 0.66849999999999243: 0.60311547578733493,\n",
       " 0.66899999999999238: 0.60277683711479846,\n",
       " 0.66949999999999232: 0.60277683711479846,\n",
       " 0.66999999999999227: 0.60260751777853028,\n",
       " 0.67049999999999221: 0.6014222824246529,\n",
       " 0.67099999999999216: 0.60176092109718926,\n",
       " 0.6714999999999921: 0.60125296308838472,\n",
       " 0.67199999999999205: 0.60091432441584824,\n",
       " 0.67249999999999199: 0.60108364375211654,\n",
       " 0.67299999999999194: 0.60125296308838472,\n",
       " 0.67349999999999188: 0.60091432441584824,\n",
       " 0.67399999999999183: 0.60159160176092108,\n",
       " 0.67449999999999177: 0.6014222824246529,\n",
       " 0.67499999999999172: 0.6014222824246529,\n",
       " 0.67549999999999166: 0.60023704707077552,\n",
       " 0.67599999999999161: 0.59989840839823905,\n",
       " 0.67649999999999155: 0.60074500507958006,\n",
       " 0.6769999999999915: 0.59989840839823905,\n",
       " 0.67749999999999144: 0.60023704707077552,\n",
       " 0.67799999999999139: 0.6004063664070437,\n",
       " 0.67849999999999133: 0.6014222824246529,\n",
       " 0.67899999999999128: 0.60125296308838472,\n",
       " 0.67949999999999122: 0.60074500507958006,\n",
       " 0.67999999999999117: 0.60006772773450723,\n",
       " 0.68049999999999111: 0.60023704707077552,\n",
       " 0.68099999999999106: 0.59972908906197087,\n",
       " 0.681499999999991: 0.59905181171689803,\n",
       " 0.68199999999999095: 0.59888249238062985,\n",
       " 0.68249999999999089: 0.59837453437182531,\n",
       " 0.68299999999999084: 0.59752793769048429,\n",
       " 0.68349999999999078: 0.59769725702675247,\n",
       " 0.68399999999999073: 0.59786657636302065,\n",
       " 0.68449999999999067: 0.5963427023366068,\n",
       " 0.68499999999999062: 0.59600406366407044,\n",
       " 0.68549999999999056: 0.59651202167287509,\n",
       " 0.68599999999999051: 0.59701997968167964,\n",
       " 0.68649999999999045: 0.59668134100914327,\n",
       " 0.6869999999999904: 0.59600406366407044,\n",
       " 0.68749999999999034: 0.59617338300033862,\n",
       " 0.68799999999999029: 0.59600406366407044,\n",
       " 0.68849999999999023: 0.5953267863189976,\n",
       " 0.68899999999999018: 0.59498814764646124,\n",
       " 0.68949999999999012: 0.59414155096512022}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_results = {}\n",
    "\n",
    "for threshold in np.arange(0.60, 0.69, step=0.0005):\n",
    "    prediction = train['avg_similarity_scaled'].apply(lambda x: x < threshold)\n",
    "    avg_results[threshold] = (train['different_author'] == prediction).mean()\n",
    "\n",
    "avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59759862778730699"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_avg_threshold = 0.6685\n",
    "\n",
    "prediction = val['avg_similarity_scaled'].apply(lambda x: x < opt_avg_threshold)\n",
    "(val['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Use only BEST k features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BEST = [ 1,   3,   5,   6,   7,  11,  46,  47,  52,  53,  55,  56,  57,\n",
    "        59,  95,  99, 110, 111, 119, 120, 121, 122, 139, 152, 173, 174,\n",
    "       186, 198, 199, 201, 202, 236, 241, 263, 279, 283, 290, 294, 310,\n",
    "       325, 328, 337, 341, 342, 344, 349, 350, 352, 354, 362, 364, 366,\n",
    "       368, 402, 403, 409, 410, 416, 417, 425, 426, 428, 440, 446, 451,\n",
    "       455, 457, 471, 482, 492, 496, 499, 501, 502, 503, 507, 508, 514,\n",
    "       515, 516, 524, 531, 544, 552, 561, 562, 563, 571, 572, 589, 592,\n",
    "       593, 594, 603, 608, 613, 614, 631, 635, 639, 641, 658, 659, 665,\n",
    "       676, 683, 684, 690, 694, 699, 704, 707, 710, 712, 722, 725, 726,\n",
    "       729, 732, 734, 750, 752, 753, 754, 758, 766, 772, 773, 775, 789,\n",
    "       793, 797, 808, 812, 813, 839, 846, 850, 866, 870, 882, 884, 894,\n",
    "       898, 902, 908, 914, 918, 919, 925]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-174-bfed4356fb10>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-174-bfed4356fb10>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    def similarities(vectors):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# def min_max_similarity_best(vectors):\n",
    "#     numerator = 0\n",
    "#     denominator = 0\n",
    "    \n",
    "#     for i in BEST:\n",
    "#         a_feature = vectors['A_{}'.format(i)]\n",
    "#         b_feature = vectors['B_{}'.format(i)]\n",
    "#         numerator += min(a_feature, b_feature)\n",
    "#         denominator += max(a_feature, b_feature)\n",
    "    \n",
    "#     return numerator / denominator\n",
    "\n",
    "# def cosine_similarity_best(vectors):\n",
    "#     a = [vectors['A_{}'.format(i)] for i in BEST]\n",
    "#     b = [vectors['B_{}'.format(i)] for i in BEST]\n",
    "\n",
    "#     return 1 - cosine(a, b)\n",
    "\n",
    "# def braycurtis_similarity(vectors):\n",
    "#     a = [vectors['A_{}'.format(i)] for i in BEST]\n",
    "#     b = [vectors['B_{}'.format(i)] for i in BEST]\n",
    "\n",
    "#     return 1 - braycurtis(a, b)\n",
    "\n",
    "# def canberra_distance(vectors):\n",
    "#     a = [vectors['A_{}'.format(i)] for i in BEST]\n",
    "#     b = [vectors['B_{}'.format(i)] for i in BEST]\n",
    "\n",
    "#     return canberra(a, b)\n",
    "\n",
    "# def cityblock_distance(vectors):\n",
    "#     a = [vectors['A_{}'.format(i)] for i in BEST]\n",
    "#     b = [vectors['B_{}'.format(i)] for i in BEST]\n",
    "\n",
    "#     return cityblock(a, b) \n",
    "\n",
    "# def chebyshev_distance(vectors):\n",
    "#     a = [vectors['A_{}'.format(i)] for i in BEST]\n",
    "#     b = [vectors['B_{}'.format(i)] for i in BEST]\n",
    "\n",
    "#     return chebyshev(a, b)  \n",
    "\n",
    "# def minkowski_2_distance(vectors):\n",
    "#     a = [vectors['A_{}'.format(i)] for i in BEST]\n",
    "#     b = [vectors['B_{}'.format(i)] for i in BEST]\n",
    "\n",
    "#     return minkowski(a, b, p=2)\n",
    "\n",
    "# def minkowski_3_distance(vectors):\n",
    "#     a = [vectors['A_{}'.format(i)] for i in BEST]\n",
    "#     b = [vectors['B_{}'.format(i)] for i in BEST]\n",
    "\n",
    "#     return minkowski(a, b, p=3)\n",
    "\n",
    "\n",
    "    \n",
    "# train['min_max_similarity_BEST'] = train.apply(lambda vectors: min_max_similarity_best(vectors[:-1]), axis=1)\n",
    "# val['min_max_similarity_BEST'] = val.apply(lambda vectors: min_max_similarity_best(vectors[:-1]), axis=1)\n",
    "\n",
    "# train['cos_similarity_BEST'] = train.apply(lambda vectors: cosine_similarity_best(vectors[:-1]), axis=1)\n",
    "# val['cos_similarity_BEST'] = val.apply(lambda vectors: cosine_similarity_best(vectors[:-1]), axis=1)\n",
    "\n",
    "# train['bray_similarity_BEST'] = train.apply(lambda vectors: braycurtis_similarity(vectors[:-1]), axis=1)\n",
    "# val['bray_similarity_BEST'] = val.apply(lambda vectors: braycurtis_similarity(vectors[:-1]), axis=1)\n",
    "\n",
    "# train['canberra_distance_BEST'] = train.apply(lambda vectors: canberra_distance(vectors[:-1]), axis=1)\n",
    "# val['canberra_distance_BEST'] = val.apply(lambda vectors: canberra_distance(vectors[:-1]), axis=1)\n",
    "\n",
    "# train['cityblock_distance_BEST'] = train.apply(lambda vectors: cityblock_distance(vectors[:-1]), axis=1)\n",
    "# val['cityblock_distance_BEST'] = val.apply(lambda vectors: cityblock_distance(vectors[:-1]), axis=1)\n",
    "\n",
    "# train['chebyshev_distance_BEST'] = train.apply(lambda vectors: chebyshev_distance(vectors[:-1]), axis=1)\n",
    "# val['chebyshev_distance_BEST'] = val.apply(lambda vectors: chebyshev_distance(vectors[:-1]), axis=1)\n",
    "\n",
    "# train['minkowski_2_distance_BEST'] = train.apply(lambda vectors: minkowski_2_distance(vectors[:-1]), axis=1)\n",
    "# val['minkowski_2_distance_BEST'] = val.apply(lambda vectors: minkowski_2_distance(vectors[:-1]), axis=1)\n",
    "\n",
    "# train['minkowski_3_distance_BEST'] = train.apply(lambda vectors: minkowski_3_distance(vectors[:-1]), axis=1)\n",
    "# val['minkowski_3_distance_BEST'] = val.apply(lambda vectors: minkowski_3_distance(vectors[:-1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minmax(a, b):\n",
    "    return sum(np.minimum(a, b)) / sum(np.maximum(a, b))\n",
    "\n",
    "def similarities(vectors):\n",
    "    a = [vectors['A_{}'.format(i)] for i in BEST]\n",
    "    b = [vectors['B_{}'.format(i)] for i in BEST]\n",
    "    \n",
    "    return (minmax(a,b), #minmax similarity\n",
    "            cosine(a, b),\n",
    "            braycurtis(a, b),\n",
    "            canberra(a, b),\n",
    "            cityblock(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity_measures = ['minmax_similarity', 'cosine_distance', 'braycurtis_distance',\n",
    "                               'canberra_distance', 'cityblock_distance']\n",
    "\n",
    "# computing train similarity measures\n",
    "train_similarities = train.apply(lambda vectors: similarities(vectors), axis=1).apply(pd.Series)\n",
    "train_similarities.columns = similarity_measures\n",
    "train_similarities['different_author'] = train['different_author']\n",
    "\n",
    "# computing val similarity measures\n",
    "val_similarities = val.apply(lambda vectors: similarities(vectors), axis=1).apply(pd.Series)\n",
    "val_similarities.columns = similarity_measures\n",
    "val_similarities['different_author'] = val['different_author']\n",
    "\n",
    "del train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minmax_similarity</th>\n",
       "      <th>cosine_distance</th>\n",
       "      <th>braycurtis_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>cityblock_distance</th>\n",
       "      <th>different_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.462521</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.367502</td>\n",
       "      <td>50.275361</td>\n",
       "      <td>9.875315</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.451114</td>\n",
       "      <td>0.217858</td>\n",
       "      <td>0.378251</td>\n",
       "      <td>51.071265</td>\n",
       "      <td>10.169369</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.453668</td>\n",
       "      <td>0.194304</td>\n",
       "      <td>0.375830</td>\n",
       "      <td>40.643491</td>\n",
       "      <td>9.433641</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.444428</td>\n",
       "      <td>0.201608</td>\n",
       "      <td>0.384631</td>\n",
       "      <td>57.348183</td>\n",
       "      <td>11.515821</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.501943</td>\n",
       "      <td>0.164008</td>\n",
       "      <td>0.331609</td>\n",
       "      <td>61.360097</td>\n",
       "      <td>11.091895</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   minmax_similarity  cosine_distance  braycurtis_distance  canberra_distance  \\\n",
       "0           0.462521         0.212500             0.367502          50.275361   \n",
       "1           0.451114         0.217858             0.378251          51.071265   \n",
       "2           0.453668         0.194304             0.375830          40.643491   \n",
       "3           0.444428         0.201608             0.384631          57.348183   \n",
       "4           0.501943         0.164008             0.331609          61.360097   \n",
       "\n",
       "   cityblock_distance  different_author  \n",
       "0            9.875315              True  \n",
       "1           10.169369              True  \n",
       "2            9.433641              True  \n",
       "3           11.515821             False  \n",
       "4           11.091895             False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minmax_similarity</th>\n",
       "      <th>cosine_distance</th>\n",
       "      <th>braycurtis_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>cityblock_distance</th>\n",
       "      <th>different_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.456671</td>\n",
       "      <td>0.212586</td>\n",
       "      <td>0.372993</td>\n",
       "      <td>50.842291</td>\n",
       "      <td>10.248057</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.434174</td>\n",
       "      <td>0.278934</td>\n",
       "      <td>0.394531</td>\n",
       "      <td>49.250785</td>\n",
       "      <td>11.453147</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.418594</td>\n",
       "      <td>0.227410</td>\n",
       "      <td>0.409847</td>\n",
       "      <td>50.624078</td>\n",
       "      <td>10.600369</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.365536</td>\n",
       "      <td>0.280048</td>\n",
       "      <td>0.464626</td>\n",
       "      <td>53.736054</td>\n",
       "      <td>12.144423</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.461470</td>\n",
       "      <td>0.193960</td>\n",
       "      <td>0.368486</td>\n",
       "      <td>50.968624</td>\n",
       "      <td>10.488887</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   minmax_similarity  cosine_distance  braycurtis_distance  canberra_distance  \\\n",
       "0           0.456671         0.212586             0.372993          50.842291   \n",
       "1           0.434174         0.278934             0.394531          49.250785   \n",
       "2           0.418594         0.227410             0.409847          50.624078   \n",
       "3           0.365536         0.280048             0.464626          53.736054   \n",
       "4           0.461470         0.193960             0.368486          50.968624   \n",
       "\n",
       "   cityblock_distance  different_author  \n",
       "0           10.248057              True  \n",
       "1           11.453147              True  \n",
       "2           10.600369             False  \n",
       "3           12.144423             False  \n",
       "4           10.488887              True  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_similarities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST minmax sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.478304\n",
       "True     0.433523\n",
       "Name: min_max_similarity_BEST, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['min_max_similarity_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.472529\n",
       "True     0.436617\n",
       "Name: min_max_similarity_BEST, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['min_max_similarity_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.41999999999999998: 0.61378259397223167,\n",
       " 0.42249999999999999: 0.61716898069759563,\n",
       " 0.42499999999999999: 0.62428039282086012,\n",
       " 0.42749999999999999: 0.63037588892651542,\n",
       " 0.42999999999999999: 0.63410091432441584,\n",
       " 0.4325: 0.63562478835082969,\n",
       " 0.435: 0.63714866237724344,\n",
       " 0.4375: 0.63918049441246194,\n",
       " 0.44: 0.64696918388079916,\n",
       " 0.4425: 0.65086352861496788,\n",
       " 0.44500000000000001: 0.6525567219776498,\n",
       " 0.44750000000000001: 0.65289536065018627,\n",
       " 0.45000000000000001: 0.65306467998645445,\n",
       " 0.45250000000000001: 0.65509651202167285,\n",
       " 0.45500000000000002: 0.6556044700304775,\n",
       " 0.45750000000000002: 0.65780562140196408,\n",
       " 0.46000000000000002: 0.65729766339315954,\n",
       " 0.46250000000000002: 0.65916017609210975,\n",
       " 0.46500000000000002: 0.66153064679986451,\n",
       " 0.46750000000000003: 0.66153064679986451,\n",
       " 0.47000000000000003: 0.65831357941076873,\n",
       " 0.47250000000000003: 0.65475787334913649,\n",
       " 0.47500000000000003: 0.65424991534033183,\n",
       " 0.47750000000000004: 0.65204876396884526,\n",
       " 0.48000000000000004: 0.65069420927869959,\n",
       " 0.48250000000000004: 0.64933965458855403,\n",
       " 0.48500000000000004: 0.64849305790721301,\n",
       " 0.48750000000000004: 0.64747714188960381,\n",
       " 0.49000000000000005: 0.64544530985438542,\n",
       " 0.49250000000000005: 0.63968845242126648,\n",
       " 0.49500000000000005: 0.63748730104977991,\n",
       " 0.49750000000000005: 0.63393159498814766,\n",
       " 0.5: 0.63173044361666097}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_BEST = {}\n",
    "\n",
    "for threshold in np.arange(0.42, 0.50, step=0.0025):\n",
    "    prediction = train_similarities['min_max_similarity_BEST'].apply(lambda x: x < threshold)\n",
    "    results_BEST[threshold] = (train_similarities['different_author'] == prediction).mean()\n",
    "    \n",
    "results_BEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62572898799313892"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_BEST_threshold = 0.465\n",
    "\n",
    "prediction = val['min_max_similarity_BEST'].apply(lambda x: x < opt_BEST_threshold)\n",
    "(val['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST cos sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.780522\n",
       "True     0.743643\n",
       "Name: cos_similarity_BEST, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['cos_similarity_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.774388\n",
       "True     0.750337\n",
       "Name: cos_similarity_BEST, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['cos_similarity_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.73999999999999999: 0.60497798848628515,\n",
       " 0.74049999999999994: 0.60582458516762616,\n",
       " 0.74099999999999988: 0.60667118184896718,\n",
       " 0.74149999999999983: 0.606501862512699,\n",
       " 0.74199999999999977: 0.6063325431764307,\n",
       " 0.74249999999999972: 0.606501862512699,\n",
       " 0.74299999999999966: 0.60734845919404001,\n",
       " 0.74349999999999961: 0.60700982052150354,\n",
       " 0.74399999999999955: 0.60717913985777172,\n",
       " 0.7444999999999995: 0.60751777853030819,\n",
       " 0.74499999999999944: 0.60853369454791739,\n",
       " 0.74549999999999939: 0.60836437521164921,\n",
       " 0.74599999999999933: 0.60887233322045375,\n",
       " 0.74649999999999928: 0.60870301388418557,\n",
       " 0.74699999999999922: 0.60921097189299023,\n",
       " 0.74749999999999917: 0.60887233322045375,\n",
       " 0.74799999999999911: 0.60904165255672194,\n",
       " 0.74849999999999905: 0.60921097189299023,\n",
       " 0.748999999999999: 0.60938029122925841,\n",
       " 0.74949999999999894: 0.60921097189299023,\n",
       " 0.74999999999999889: 0.60971892990179477,\n",
       " 0.75049999999999883: 0.61056552658313579,\n",
       " 0.75099999999999878: 0.60988824923806295,\n",
       " 0.75149999999999872: 0.60971892990179477,\n",
       " 0.75199999999999867: 0.61124280392820862,\n",
       " 0.75249999999999861: 0.61225871994581782,\n",
       " 0.75299999999999856: 0.61293599729089066,\n",
       " 0.7534999999999985: 0.61225871994581782,\n",
       " 0.75399999999999845: 0.6134439552996952,\n",
       " 0.75449999999999839: 0.61462919065357269,\n",
       " 0.75499999999999834: 0.61462919065357269,\n",
       " 0.75549999999999828: 0.61378259397223167,\n",
       " 0.75599999999999823: 0.61429055198103621,\n",
       " 0.75649999999999817: 0.61412123264476803,\n",
       " 0.75699999999999812: 0.61429055198103621,\n",
       " 0.75749999999999806: 0.61429055198103621,\n",
       " 0.75799999999999801: 0.6134439552996952,\n",
       " 0.75849999999999795: 0.61378259397223167,\n",
       " 0.7589999999999979: 0.61293599729089066,\n",
       " 0.75949999999999784: 0.61395191330849985,\n",
       " 0.75999999999999779: 0.61378259397223167,\n",
       " 0.76049999999999773: 0.61327463596342702,\n",
       " 0.76099999999999768: 0.61429055198103621,\n",
       " 0.76149999999999762: 0.61412123264476803,\n",
       " 0.76199999999999757: 0.61378259397223167,\n",
       " 0.76249999999999751: 0.61327463596342702,\n",
       " 0.76299999999999746: 0.61327463596342702,\n",
       " 0.7634999999999974: 0.61259735861835418,\n",
       " 0.76399999999999735: 0.61293599729089066,\n",
       " 0.76449999999999729: 0.61293599729089066,\n",
       " 0.76499999999999724: 0.61361327463596338,\n",
       " 0.76549999999999718: 0.61429055198103621,\n",
       " 0.76599999999999713: 0.61462919065357269,\n",
       " 0.76649999999999707: 0.61530646799864541,\n",
       " 0.76699999999999702: 0.61496782932610905,\n",
       " 0.76749999999999696: 0.61412123264476803,\n",
       " 0.76799999999999691: 0.61479850998984087,\n",
       " 0.76849999999999685: 0.61598374534371825,\n",
       " 0.7689999999999968: 0.61479850998984087,\n",
       " 0.76949999999999674: 0.61513714866237723,\n",
       " 0.76999999999999669: 0.61615306467998643,\n",
       " 0.77049999999999663: 0.61733830003386392,\n",
       " 0.77099999999999658: 0.61683034202505926,\n",
       " 0.77149999999999652: 0.61615306467998643,\n",
       " 0.77199999999999647: 0.61699966136132744,\n",
       " 0.77249999999999641: 0.61615306467998643,\n",
       " 0.77299999999999636: 0.61547578733491359,\n",
       " 0.7734999999999963: 0.6144598713173044,\n",
       " 0.77399999999999625: 0.6144598713173044,\n",
       " 0.77449999999999619: 0.61564510667118189,\n",
       " 0.77499999999999614: 0.61547578733491359,\n",
       " 0.77549999999999608: 0.61547578733491359,\n",
       " 0.77599999999999603: 0.61615306467998643,\n",
       " 0.77649999999999597: 0.61615306467998643,\n",
       " 0.77699999999999592: 0.61615306467998643,\n",
       " 0.77749999999999586: 0.61666102268879108,\n",
       " 0.77799999999999581: 0.6164917033525229,\n",
       " 0.77849999999999575: 0.61784625804266846,\n",
       " 0.7789999999999957: 0.61683034202505926,\n",
       " 0.77949999999999564: 0.6175076193701321,\n",
       " 0.77999999999999559: 0.61733830003386392,\n",
       " 0.78049999999999553: 0.61767693870640028,\n",
       " 0.78099999999999548: 0.61835421605147312,\n",
       " 0.78149999999999542: 0.61886217406027766,\n",
       " 0.78199999999999537: 0.61937013206908231,\n",
       " 0.78249999999999531: 0.61987809007788686,\n",
       " 0.78299999999999526: 0.62072468675922787,\n",
       " 0.7834999999999952: 0.61987809007788686,\n",
       " 0.78399999999999515: 0.61953945140535049,\n",
       " 0.78449999999999509: 0.61886217406027766,\n",
       " 0.78499999999999504: 0.61920081273281413,\n",
       " 0.78549999999999498: 0.61920081273281413,\n",
       " 0.78599999999999492: 0.61937013206908231,\n",
       " 0.78649999999999487: 0.61937013206908231,\n",
       " 0.78699999999999481: 0.61869285472400948,\n",
       " 0.78749999999999476: 0.61903149339654584,\n",
       " 0.7879999999999947: 0.61801557737893664,\n",
       " 0.78849999999999465: 0.6185235353877413,\n",
       " 0.78899999999999459: 0.61716898069759563,\n",
       " 0.78949999999999454: 0.61615306467998643,\n",
       " 0.78999999999999448: 0.61496782932610905,\n",
       " 0.79049999999999443: 0.61513714866237723,\n",
       " 0.79099999999999437: 0.61513714866237723,\n",
       " 0.79149999999999432: 0.61683034202505926,\n",
       " 0.79199999999999426: 0.61699966136132744,\n",
       " 0.79249999999999421: 0.61767693870640028,\n",
       " 0.79299999999999415: 0.61784625804266846,\n",
       " 0.7934999999999941: 0.61767693870640028,\n",
       " 0.79399999999999404: 0.61767693870640028,\n",
       " 0.79449999999999399: 0.61818489671520482,\n",
       " 0.79499999999999393: 0.6175076193701321,\n",
       " 0.79549999999999388: 0.61699966136132744,\n",
       " 0.79599999999999382: 0.61716898069759563,\n",
       " 0.79649999999999377: 0.61598374534371825,\n",
       " 0.79699999999999371: 0.61632238401625461,\n",
       " 0.79749999999999366: 0.61564510667118189,\n",
       " 0.7979999999999936: 0.61530646799864541,\n",
       " 0.79849999999999355: 0.61479850998984087,\n",
       " 0.79899999999999349: 0.61429055198103621,\n",
       " 0.79949999999999344: 0.6134439552996952,\n",
       " 0.79999999999999338: 0.61175076193701317,\n",
       " 0.80049999999999333: 0.61175076193701317,\n",
       " 0.80099999999999327: 0.61073484591940397,\n",
       " 0.80149999999999322: 0.61022688791059942,\n",
       " 0.80199999999999316: 0.60988824923806295,\n",
       " 0.80249999999999311: 0.60971892990179477,\n",
       " 0.80299999999999305: 0.60971892990179477,\n",
       " 0.803499999999993: 0.60904165255672194,\n",
       " 0.80399999999999294: 0.60887233322045375,\n",
       " 0.80449999999999289: 0.60785641720284456,\n",
       " 0.80499999999999283: 0.60768709786657638,\n",
       " 0.80549999999999278: 0.60751777853030819,\n",
       " 0.80599999999999272: 0.60751777853030819,\n",
       " 0.80649999999999267: 0.60599390450389434,\n",
       " 0.80699999999999261: 0.6063325431764307,\n",
       " 0.80749999999999256: 0.606501862512699,\n",
       " 0.8079999999999925: 0.606501862512699,\n",
       " 0.80849999999999245: 0.60548594649508969,\n",
       " 0.80899999999999239: 0.60599390450389434,\n",
       " 0.80949999999999234: 0.60582458516762616,\n",
       " 0.80999999999999228: 0.60582458516762616,\n",
       " 0.81049999999999223: 0.60447003047748049,\n",
       " 0.81099999999999217: 0.60379275313240777,\n",
       " 0.81149999999999212: 0.60362343379613947,\n",
       " 0.81199999999999206: 0.60328479512360311,\n",
       " 0.81249999999999201: 0.60226887910599392,\n",
       " 0.81299999999999195: 0.60159160176092108,\n",
       " 0.8134999999999919: 0.60209955976972573,\n",
       " 0.81399999999999184: 0.60176092109718926,\n",
       " 0.81449999999999179: 0.60091432441584824,\n",
       " 0.81499999999999173: 0.6004063664070437,\n",
       " 0.81549999999999168: 0.59972908906197087,\n",
       " 0.81599999999999162: 0.59955976972570268,\n",
       " 0.81649999999999157: 0.59837453437182531,\n",
       " 0.81699999999999151: 0.59888249238062985,\n",
       " 0.81749999999999146: 0.59837453437182531,\n",
       " 0.8179999999999914: 0.59769725702675247,\n",
       " 0.81849999999999135: 0.59685066034541145,\n",
       " 0.81899999999999129: 0.59701997968167964,\n",
       " 0.81949999999999124: 0.59685066034541145}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_BEST_cos = {}\n",
    "\n",
    "for threshold in np.arange(0.74, 0.82, step=0.0005):\n",
    "    prediction = train_similarities['cos_similarity_BEST'].apply(lambda x: x < threshold)\n",
    "    results_BEST_cos[threshold] = (train_similarities['different_author'] == prediction).mean()\n",
    "    \n",
    "results_BEST_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57838765008576332"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_BEST_cos_threshold = 0.795\n",
    "\n",
    "prediction = val['cos_similarity_BEST'].apply(lambda x: x < opt_BEST_cos_threshold)\n",
    "(val['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST braycurtis sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.644707\n",
       "True     0.602949\n",
       "Name: bray_similarity_BEST, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['bray_similarity_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.63911\n",
       "True     0.60586\n",
       "Name: bray_similarity_BEST, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['bray_similarity_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.59999999999999998: 0.63342363697934301,\n",
       " 0.60249999999999992: 0.63427023366068402,\n",
       " 0.60499999999999987: 0.63748730104977991,\n",
       " 0.60749999999999982: 0.63934981374873012,\n",
       " 0.60999999999999976: 0.64307483914663055,\n",
       " 0.61249999999999971: 0.65187944463257708,\n",
       " 0.61499999999999966: 0.6515408059600406,\n",
       " 0.6174999999999996: 0.65408059600406365,\n",
       " 0.61999999999999955: 0.65408059600406365,\n",
       " 0.6224999999999995: 0.65594310870301387,\n",
       " 0.62499999999999944: 0.65458855401286831,\n",
       " 0.62749999999999939: 0.65797494073823226,\n",
       " 0.62999999999999934: 0.65729766339315954,\n",
       " 0.63249999999999929: 0.65932949542837793,\n",
       " 0.63499999999999923: 0.66203860480866916,\n",
       " 0.63749999999999918: 0.66085336945479178,\n",
       " 0.63999999999999913: 0.65695902472062306,\n",
       " 0.64249999999999907: 0.65509651202167285,\n",
       " 0.64499999999999902: 0.65408059600406365,\n",
       " 0.64749999999999897: 0.65221808330511344,\n",
       " 0.64999999999999891: 0.64950897392482221,\n",
       " 0.65249999999999886: 0.64900101591601755,\n",
       " 0.65499999999999881: 0.64781578056214018,\n",
       " 0.65749999999999875: 0.6456146291906536,\n",
       " 0.6599999999999987: 0.6395191330849983}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_BEST_braycurtis = {}\n",
    "\n",
    "for threshold in np.arange(0.60, 0.66, step=0.0025):\n",
    "    prediction = train_similarities['bray_similarity_BEST'].apply(lambda x: x < threshold)\n",
    "    results_BEST_braycurtis[threshold] = (train_similarities['different_author'] == prediction).mean()\n",
    "    \n",
    "results_BEST_braycurtis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62744425385934821"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_BEST_threshold_braycurtis = 0.645\n",
    "\n",
    "prediction = val['bray_similarity_BEST'].apply(lambda x: x < opt_BEST_threshold_braycurtis)\n",
    "(val['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST canberra dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    40.764737\n",
       "True     50.644562\n",
       "Name: canberra_similarity_BEST, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['canberra_distance_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    41.758544\n",
       "True     50.619321\n",
       "Name: canberra_similarity_BEST, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['canberra_distance_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{40.0: 0.66220792414493734,\n",
       " 40.5: 0.66813410091432446,\n",
       " 41.0: 0.67033525228581103,\n",
       " 41.5: 0.6737216390111751,\n",
       " 42.0: 0.67592279038266168,\n",
       " 42.5: 0.67829326109041654,\n",
       " 43.0: 0.68235692516085333,\n",
       " 43.5: 0.683711479850999,\n",
       " 44.0: 0.68252624449712163,\n",
       " 44.5: 0.68523535387741275,\n",
       " 45.0: 0.68506603454114456,\n",
       " 45.5: 0.68320352184219435,\n",
       " 46.0: 0.68269556383338981,\n",
       " 46.5: 0.68591263122248558,\n",
       " 47.0: 0.68777514392143579,\n",
       " 47.5: 0.6857433118862174,\n",
       " 48.0: 0.68455807653234002,\n",
       " 48.5: 0.68269556383338981,\n",
       " 49.0: 0.67846258042668472,\n",
       " 49.5: 0.67389095834744328,\n",
       " 50.0: 0.67236708432102943,\n",
       " 50.5: 0.66694886556044697,\n",
       " 51.0: 0.6616999661361328,\n",
       " 51.5: 0.65628174737555034}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_BEST_canberra = {}\n",
    "\n",
    "for threshold in np.arange(40, 52, step=0.5):\n",
    "    prediction = train_similarities['canberra_distance_BEST'].apply(lambda x: x > threshold)\n",
    "    results_BEST_canberra[threshold] = (train_similarities['different_author'] == prediction).mean()\n",
    "    \n",
    "results_BEST_canberra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66929674099485426"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_BEST_threshold_canberra = 46\n",
    "\n",
    "prediction = val['canberra_distance_BEST'].apply(lambda x: x > opt_BEST_threshold_canberra)\n",
    "(val['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST cityblock dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False     9.807928\n",
       "True     11.533602\n",
       "Name: cityblock_distance_BEST, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['cityblock_distance_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    10.160012\n",
       "True     11.401366\n",
       "Name: cityblock_distance_BEST, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.groupby('different_author').mean()['cityblock_distance_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9.0: 0.62292583813071456,\n",
       " 9.125: 0.6305452082627836,\n",
       " 9.25: 0.63664070436843889,\n",
       " 9.375: 0.64442939383677611,\n",
       " 9.5: 0.65069420927869959,\n",
       " 9.625: 0.6576363020656959,\n",
       " 9.75: 0.66085336945479178,\n",
       " 9.875: 0.66644090755164243,\n",
       " 10.0: 0.66694886556044697,\n",
       " 10.125: 0.6716898069759567,\n",
       " 10.25: 0.67202844564849307,\n",
       " 10.375: 0.67490687436505248,\n",
       " 10.5: 0.67219776498476125,\n",
       " 10.625: 0.66931933626820184,\n",
       " 10.75: 0.66711818489671515,\n",
       " 10.875: 0.66491703352522857,\n",
       " 11.0: 0.66474771418896039,\n",
       " 11.125: 0.662715882153742,\n",
       " 11.25: 0.66017609210971895,\n",
       " 11.375: 0.65780562140196408,\n",
       " 11.5: 0.65035557060616322,\n",
       " 11.625: 0.64595326786318996,\n",
       " 11.75: 0.64121232644768034,\n",
       " 11.875: 0.63596342702336606}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_BEST_cityblock = {}\n",
    "\n",
    "for threshold in np.arange(9, 12, step=0.125):\n",
    "    prediction = train_similarities['cityblock_distance_BEST'].apply(lambda x: x > threshold)\n",
    "    results_BEST_cityblock[threshold] = (train_similarities['different_author'] == prediction).mean()\n",
    "    \n",
    "results_BEST_cityblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62058319039451115"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_BEST_threshold_cityblock = 10.5\n",
    "\n",
    "prediction = val['cityblock_distance_BEST'].apply(lambda x: x > opt_BEST_threshold_cityblock)\n",
    "(val['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST chebyshev dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.543820\n",
       "True     0.584682\n",
       "Name: chebyshev_distance_BEST, dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['chebyshev_distance_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.551264\n",
       "True     0.571469\n",
       "Name: chebyshev_distance_BEST, dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.groupby('different_author').mean()['chebyshev_distance_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: 0.55231967490687439,\n",
       " 0.50249999999999995: 0.55299695225194723,\n",
       " 0.50499999999999989: 0.55536742295970198,\n",
       " 0.50749999999999984: 0.55604470030477482,\n",
       " 0.50999999999999979: 0.55672197764984765,\n",
       " 0.51249999999999973: 0.55756857433118867,\n",
       " 0.51499999999999968: 0.55875380968506605,\n",
       " 0.51749999999999963: 0.56095496105655263,\n",
       " 0.51999999999999957: 0.56044700304774808,\n",
       " 0.52249999999999952: 0.55858449034879787,\n",
       " 0.52499999999999947: 0.55926176769387059,\n",
       " 0.52749999999999941: 0.5582458516762614,\n",
       " 0.52999999999999936: 0.55993904503894343,\n",
       " 0.53249999999999931: 0.56112428039282081,\n",
       " 0.53499999999999925: 0.56061632238401626,\n",
       " 0.5374999999999992: 0.55909244835760241,\n",
       " 0.53999999999999915: 0.55993904503894343,\n",
       " 0.54249999999999909: 0.55943108703013888,\n",
       " 0.54499999999999904: 0.55739925499492038,\n",
       " 0.54749999999999899: 0.55672197764984765,\n",
       " 0.54999999999999893: 0.55384354893328813,\n",
       " 0.55249999999999888: 0.55265831357941075,\n",
       " 0.55499999999999883: 0.55231967490687439,\n",
       " 0.55749999999999877: 0.55248899424314257,\n",
       " 0.55999999999999872: 0.55231967490687439,\n",
       " 0.56249999999999867: 0.55299695225194723,\n",
       " 0.56499999999999861: 0.55130375888926519,\n",
       " 0.56749999999999856: 0.5501185235353877,\n",
       " 0.56999999999999851: 0.550287842871656,\n",
       " 0.57249999999999845: 0.54944124619031498,\n",
       " 0.5749999999999984: 0.550287842871656,\n",
       " 0.57749999999999835: 0.54994920419911952,\n",
       " 0.57999999999999829: 0.5511344395529969,\n",
       " 0.58249999999999824: 0.55198103623433792,\n",
       " 0.58499999999999819: 0.55147307822553338,\n",
       " 0.58749999999999813: 0.54825601083643749}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_BEST_chebyshev = {}\n",
    "\n",
    "for threshold in np.arange(0.5, 0.59, step=0.0025):\n",
    "    prediction = train_similarities['chebyshev_distance_BEST'].apply(lambda x: x > threshold)\n",
    "    results_BEST_chebyshev[threshold] = (train_similarities['different_author'] == prediction).mean()\n",
    "    \n",
    "results_BEST_chebyshev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53138936535162951"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_BEST_threshold_chebyshev = 0.53\n",
    "\n",
    "prediction = val['chebyshev_distance_BEST'].apply(lambda x: x > opt_BEST_threshold_chebyshev)\n",
    "(val['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST minkowski_2 dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    1.501683\n",
       "True     1.644552\n",
       "Name: minkowski_2_distance_BEST, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['minkowski_2_distance_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    1.538390\n",
       "True     1.617354\n",
       "Name: minkowski_2_distance_BEST, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.groupby('different_author').mean()['minkowski_2_distance_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.48: 0.62038604808669151,\n",
       " 1.4849999999999999: 0.62004740941415515,\n",
       " 1.4899999999999998: 0.62089400609549605,\n",
       " 1.4949999999999997: 0.61818489671520482,\n",
       " 1.4999999999999996: 0.61767693870640028,\n",
       " 1.5049999999999994: 0.61699966136132744,\n",
       " 1.5099999999999993: 0.61632238401625461,\n",
       " 1.5149999999999992: 0.61581442600745007,\n",
       " 1.5199999999999991: 0.61716898069759563,\n",
       " 1.524999999999999: 0.6164917033525229,\n",
       " 1.5299999999999989: 0.61429055198103621,\n",
       " 1.5349999999999988: 0.6144598713173044,\n",
       " 1.5399999999999987: 0.61462919065357269,\n",
       " 1.5449999999999986: 0.61429055198103621,\n",
       " 1.5499999999999985: 0.61479850998984087,\n",
       " 1.5549999999999984: 0.61412123264476803,\n",
       " 1.5599999999999983: 0.6144598713173044,\n",
       " 1.5649999999999982: 0.612428039282086,\n",
       " 1.5699999999999981: 0.61327463596342702,\n",
       " 1.574999999999998: 0.61479850998984087,\n",
       " 1.5799999999999979: 0.61310531662715884,\n",
       " 1.5849999999999977: 0.61073484591940397,\n",
       " 1.5899999999999976: 0.60954961056552659,\n",
       " 1.5949999999999975: 0.60921097189299023,\n",
       " 1.5999999999999974: 0.60768709786657638,\n",
       " 1.6049999999999973: 0.606501862512699,\n",
       " 1.6099999999999972: 0.60396207246867595,\n",
       " 1.6149999999999971: 0.60328479512360311,\n",
       " 1.619999999999997: 0.60193024043345755,\n",
       " 1.6249999999999969: 0.60193024043345755,\n",
       " 1.6299999999999968: 0.60159160176092108,\n",
       " 1.6349999999999967: 0.59905181171689803,\n",
       " 1.6399999999999966: 0.59685066034541145,\n",
       " 1.6449999999999965: 0.59718929901794782}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_BEST_minkowski_2 = {}\n",
    "\n",
    "for threshold in np.arange(1.48, 1.65, step=0.005):\n",
    "    prediction = train_similarities['minkowski_2_distance_BEST'].apply(lambda x: x > threshold)\n",
    "    results_BEST_minkowski_2[threshold] = (train_similarities['different_author'] == prediction).mean()\n",
    "    \n",
    "results_BEST_minkowski_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.516295025728988"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_BEST_threshold_minkowski_2 = 1.2\n",
    "\n",
    "prediction = val['minkowski_2_distance_BEST'].apply(lambda x: x > opt_BEST_threshold_minkowski_2)\n",
    "(val['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST minkowski_3 dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.889298\n",
       "True     0.960285\n",
       "Name: minkowski_3_distance_BEST, dtype: float64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['minkowski_3_distance_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    0.907721\n",
       "True     0.941107\n",
       "Name: minkowski_3_distance_BEST, dtype: float64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.groupby('different_author').mean()['minkowski_3_distance_BEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.85999999999999999: 0.59210971892990183,\n",
       " 0.86499999999999999: 0.59143244158482899,\n",
       " 0.87: 0.59244835760243819,\n",
       " 0.875: 0.59092448357602434,\n",
       " 0.88: 0.59160176092109718,\n",
       " 0.88500000000000001: 0.59210971892990183,\n",
       " 0.89000000000000001: 0.59278699627497455,\n",
       " 0.89500000000000002: 0.59143244158482899,\n",
       " 0.90000000000000002: 0.58973924822214696,\n",
       " 0.90500000000000003: 0.58804605485946493,\n",
       " 0.91000000000000003: 0.5843210294615645,\n",
       " 0.91500000000000004: 0.58449034879783268,\n",
       " 0.92000000000000004: 0.58449034879783268,\n",
       " 0.92500000000000004: 0.58211987809007792,\n",
       " 0.93000000000000005: 0.58025736539112771,\n",
       " 0.93500000000000005: 0.57737893667456819,\n",
       " 0.94000000000000006: 0.57619370132069081,\n",
       " 0.94500000000000006: 0.57348459194039958,\n",
       " 0.95000000000000007: 0.57450050795800878,\n",
       " 0.95500000000000007: 0.5743311886217406,\n",
       " 0.96000000000000008: 0.57466982729427696,\n",
       " 0.96500000000000008: 0.5733152726041314,\n",
       " 0.97000000000000008: 0.57043684388757199,\n",
       " 0.97500000000000009: 0.57009820521503551}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_BEST_minkowski_3 = {}\n",
    "\n",
    "for threshold in np.arange(0.86, 0.98, step=0.005):\n",
    "    prediction = train_similarities['minkowski_3_distance_BEST'].apply(lambda x: x > threshold)\n",
    "    results_BEST_minkowski_3[threshold] = (train_similarities['different_author'] == prediction).mean()\n",
    "    \n",
    "results_BEST_minkowski_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55025728987993139"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_BEST_threshold_minkowski_3 = 0.89\n",
    "\n",
    "prediction = val['minkowski_3_distance_BEST'].apply(lambda x: x > opt_BEST_threshold_minkowski_3)\n",
    "(val['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chebyshev and minkowski are not included as they perform poorly \n",
    "\n",
    "similarity_measures = ['minmax_similarity', 'cosine_distance', 'braycurtis_distance',\n",
    "                               'canberra_distance', 'cityblock_distance']\n",
    "\n",
    "t_x = train_similarities[similarity_measures]\n",
    "t_y = train_similarities['different_author']\n",
    "v_x = val_similarities[similarity_measures]\n",
    "v_y = val_similarities['different_author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# standard_scaler = StandardScaler()\n",
    "# standard_scaler.fit(t_x)\n",
    "\n",
    "# minmax_scaler = MinMaxScaler()\n",
    "# minmax_scaler.fit(t_x)\n",
    "\n",
    "# t_x = minmax_scaler.transform(t_x)\n",
    "# v_x = minmax_scaler.transform(v_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.730782255334\n",
      "val accuracy:  0.69845626072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(t_x, t_y)\n",
    "\n",
    "print('train accuracy: ', (model.predict(t_x) == t_y).mean())\n",
    "print('val accuracy: ', (model.predict(v_x) == v_y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.67519843,  5.48266274,  2.52341968,  0.12660608, -0.24286138]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minmax_similarity</th>\n",
       "      <th>cosine_distance</th>\n",
       "      <th>braycurtis_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>cityblock_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.462521</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.367502</td>\n",
       "      <td>50.275361</td>\n",
       "      <td>9.875315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.451114</td>\n",
       "      <td>0.217858</td>\n",
       "      <td>0.378251</td>\n",
       "      <td>51.071265</td>\n",
       "      <td>10.169369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.453668</td>\n",
       "      <td>0.194304</td>\n",
       "      <td>0.375830</td>\n",
       "      <td>40.643491</td>\n",
       "      <td>9.433641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.444428</td>\n",
       "      <td>0.201608</td>\n",
       "      <td>0.384631</td>\n",
       "      <td>57.348183</td>\n",
       "      <td>11.515821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.501943</td>\n",
       "      <td>0.164008</td>\n",
       "      <td>0.331609</td>\n",
       "      <td>61.360097</td>\n",
       "      <td>11.091895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   minmax_similarity  cosine_distance  braycurtis_distance  canberra_distance  \\\n",
       "0           0.462521         0.212500             0.367502          50.275361   \n",
       "1           0.451114         0.217858             0.378251          51.071265   \n",
       "2           0.453668         0.194304             0.375830          40.643491   \n",
       "3           0.444428         0.201608             0.384631          57.348183   \n",
       "4           0.501943         0.164008             0.331609          61.360097   \n",
       "\n",
       "   cityblock_distance  \n",
       "0            9.875315  \n",
       "1           10.169369  \n",
       "2            9.433641  \n",
       "3           11.515821  \n",
       "4           11.091895  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.712834405689\n",
      "val accuracy:  0.670668953688\n"
     ]
    }
   ],
   "source": [
    "model_svc = SVC()\n",
    "model_svc.fit(t_x, t_y)\n",
    "\n",
    "print('train accuracy: ', (model_svc.predict(t_x) == t_y).mean())\n",
    "print('val accuracy: ', (model_svc.predict(v_x) == v_y).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use top 150 from selector + top 150 from random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BEST = [0, 1, 2, 3, 5, 6, 7, 8, 11, 15, 16, 26, 35, 42, 45, 46, 47, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n",
    "        60, 63, 66, 68, 69, 81, 84, 88, 91, 95, 97, 98, 99, 103, 108, 109, 110, 111, 112, 114, 119, 120, 121,\n",
    "        122, 123, 128, 130, 133, 136, 138, 139, 140, 146, 149, 152, 154, 160, 163, 166, 168, 170, 173, 174, 175,\n",
    "        176, 186, 187, 189, 193, 197, 198, 199, 200, 201, 202, 203, 204, 212, 213, 219, 225, 227, 232, 233, 234,\n",
    "        236, 240, 241, 242, 243, 244, 248, 251, 256, 263, 273, 276, 279, 282, 283, 288, 290, 292, 294, 299, 310,\n",
    "        317, 325, 328, 330, 331, 333, 337, 341, 342, 344, 349, 350, 352, 354, 357, 362, 364, 366, 368, 379, 402,\n",
    "        403, 409, 410, 416, 417, 425, 426, 428, 439, 440, 446, 451, 455, 456, 457, 458, 471, 473, 476, 482, 492,\n",
    "        496, 499, 501, 502, 503, 507, 508, 512, 514, 515, 516, 521, 524, 531, 537, 543, 544, 552, 561, 562, 563,\n",
    "        568, 571, 572, 589, 592, 593, 594, 603, 608, 613, 614, 627, 631, 635, 639, 641, 644, 654, 658, 659, 663,\n",
    "        665, 672, 673, 674, 675, 676, 683, 684, 690, 694, 699, 704, 707, 710, 712, 722, 725, 726, 729, 732, 734,\n",
    "        750, 752, 753, 754, 758, 766, 772, 773, 775, 783, 784, 789, 793, 797, 808, 812, 813, 826, 828, 835, 837,\n",
    "        839, 840, 846, 849, 850, 866, 870, 882, 884, 894, 898, 899, 902, 908, 909, 914, 915, 918, 919, 925]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minmax(a, b):\n",
    "    return sum(np.minimum(a, b)) / sum(np.maximum(a, b))\n",
    "\n",
    "def similarities(vectors):\n",
    "    a = [vectors['A_{}'.format(i)] for i in BEST]\n",
    "    b = [vectors['B_{}'.format(i)] for i in BEST]\n",
    "    \n",
    "    return (minmax(a,b), #minmax similarity\n",
    "            cosine(a, b),\n",
    "            braycurtis(a, b),\n",
    "            canberra(a, b),\n",
    "            cityblock(a, b))\n",
    "\n",
    "similarity_measures = ['minmax_similarity', 'cosine_distance', 'braycurtis_distance',\n",
    "                               'canberra_distance', 'cityblock_distance']\n",
    "\n",
    "# computing train similarity measures\n",
    "train_similarities = train.apply(lambda vectors: similarities(vectors), axis=1).apply(pd.Series)\n",
    "train_similarities.columns = similarity_measures\n",
    "train_similarities['different_author'] = train['different_author']\n",
    "\n",
    "# computing val similarity measures\n",
    "val_similarities = val.apply(lambda vectors: similarities(vectors), axis=1).apply(pd.Series)\n",
    "val_similarities.columns = similarity_measures\n",
    "val_similarities['different_author'] = val['different_author']\n",
    "\n",
    "del train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minmax_similarity</th>\n",
       "      <th>cosine_distance</th>\n",
       "      <th>braycurtis_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>cityblock_distance</th>\n",
       "      <th>different_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.517347</td>\n",
       "      <td>0.166647</td>\n",
       "      <td>0.318090</td>\n",
       "      <td>104.640771</td>\n",
       "      <td>20.850093</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512320</td>\n",
       "      <td>0.168996</td>\n",
       "      <td>0.322472</td>\n",
       "      <td>105.428127</td>\n",
       "      <td>21.132880</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.455313</td>\n",
       "      <td>0.241103</td>\n",
       "      <td>0.374274</td>\n",
       "      <td>87.602779</td>\n",
       "      <td>24.214927</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.518682</td>\n",
       "      <td>0.155061</td>\n",
       "      <td>0.316932</td>\n",
       "      <td>103.651134</td>\n",
       "      <td>20.392671</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.573467</td>\n",
       "      <td>0.120926</td>\n",
       "      <td>0.271079</td>\n",
       "      <td>103.058471</td>\n",
       "      <td>19.182130</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   minmax_similarity  cosine_distance  braycurtis_distance  canberra_distance  \\\n",
       "0           0.517347         0.166647             0.318090         104.640771   \n",
       "1           0.512320         0.168996             0.322472         105.428127   \n",
       "2           0.455313         0.241103             0.374274          87.602779   \n",
       "3           0.518682         0.155061             0.316932         103.651134   \n",
       "4           0.573467         0.120926             0.271079         103.058471   \n",
       "\n",
       "   cityblock_distance  different_author  \n",
       "0           20.850093              True  \n",
       "1           21.132880              True  \n",
       "2           24.214927              True  \n",
       "3           20.392671             False  \n",
       "4           19.182130             False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity_measures = ['minmax_similarity', 'cosine_distance', 'braycurtis_distance',\n",
    "                               'canberra_distance', 'cityblock_distance']\n",
    "\n",
    "t_x = train_similarities[similarity_measures]\n",
    "t_y = train_similarities['different_author']\n",
    "v_x = val_similarities[similarity_measures]\n",
    "v_y = val_similarities['different_author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.738232306129\n",
      "val accuracy:  0.714922813036\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(t_x, t_y)\n",
    "\n",
    "print('train accuracy: ', (model.predict(t_x) == t_y).mean())\n",
    "print('val accuracy: ', (model.predict(v_x) == v_y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.97790263,  4.05926716,  1.44823056,  0.11080992, -0.12487768]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST canberra dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    84.338491\n",
       "True     97.998915\n",
       "Name: canberra_distance, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarities.groupby('different_author').mean()['canberra_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "different_author\n",
       "False    85.601155\n",
       "True     97.913699\n",
       "Name: canberra_distance, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_similarities.groupby('different_author').mean()['canberra_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{83.0: 0.67795462241788007,\n",
       " 83.5: 0.67778530308161189,\n",
       " 84.0: 0.68405011852353537,\n",
       " 84.5: 0.68912969861158146,\n",
       " 85.0: 0.69268540467321371,\n",
       " 85.5: 0.69844226210633253,\n",
       " 86.0: 0.70149001015916013,\n",
       " 86.5: 0.70098205215035558,\n",
       " 87.0: 0.70470707754825601,\n",
       " 87.5: 0.70944801896376564,\n",
       " 88.0: 0.70978665763630211,\n",
       " 88.5: 0.7138503217067389,\n",
       " 89.0: 0.71554351506942093,\n",
       " 89.5: 0.71605147307822559,\n",
       " 90.0: 0.71655943108703013,\n",
       " 90.5: 0.71757534710463933,\n",
       " 91.0: 0.71808330511344398,\n",
       " 91.5: 0.71706738909583478,\n",
       " 92.0: 0.71622079241449377,\n",
       " 92.5: 0.71672875042329831,\n",
       " 93.0: 0.71808330511344398,\n",
       " 93.5: 0.71435827971554355,\n",
       " 94.0: 0.71097189299017949,\n",
       " 94.5: 0.70860142228242462,\n",
       " 95.0: 0.70606163223840157,\n",
       " 95.5: 0.7028445648493058,\n",
       " 96.0: 0.69641043007111414,\n",
       " 96.5: 0.69437859803589574,\n",
       " 97.0: 0.69319336268201825,\n",
       " 97.5: 0.69116153064679986,\n",
       " 98.0: 0.68625126989502205,\n",
       " 98.5: 0.68303420250592617}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_BEST_canberra = {}\n",
    "\n",
    "for threshold in np.arange(83, 99, step=0.5):\n",
    "    prediction = train_similarities['canberra_distance'].apply(lambda x: x > threshold)\n",
    "    results_BEST_canberra[threshold] = (train_similarities['different_author'] == prediction).mean()\n",
    "    \n",
    "results_BEST_canberra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69433962264150939"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_BEST_threshold_canberra = 91\n",
    "\n",
    "prediction = val_similarities['canberra_distance'].apply(lambda x: x > opt_BEST_threshold_canberra)\n",
    "(val_similarities['different_author'] == prediction).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
